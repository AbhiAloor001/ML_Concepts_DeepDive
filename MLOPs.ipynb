{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the data types to be dealt with?\n",
    "* Text\n",
    "* Numerical\n",
    "* Image\n",
    "* Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Name the 2 steps to be performed on raw data before feeding it into the ML algorithm.\n",
    "\n",
    "1. Data cleaning - to remove the noises & inconsistencies in the data\n",
    "2. Feature extraction (or **re-scaling** in case of **numerical data**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is feature extraction?\n",
    "\n",
    "* process of transforming raw data into a set of relevant, informative features that can be used as inputs to a ML model\n",
    "* reduces the dimensionality of the data by selecting/ transforming input variables while preserving the most important information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the need for feature extraction?\n",
    "\n",
    "Raw data is ususally :\n",
    "* high-dimensional\n",
    "* noisy or redundant\n",
    "* inappropriate for ML algos\n",
    "\n",
    "Hence, feature extraction that does :\n",
    "* reduce the number of features to simplify the model and improve computational efficiency,\n",
    "* compact representation of the data to save storage and processing time,\n",
    "* enhance accuracy by removing irrelevant or redundant features,\n",
    "* simplify the dataset to make the results easier to interpret is a much needed process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Feature extraction is crucial for building a successful ML model. Justify.\n",
    "\n",
    "||Result|What FE does?|\n",
    "|--|----|----|\n",
    "|1|Improved Accuracy|extract features that capture the most relevant information, leading to better predictions|\n",
    "|2|Reduced Overfitting|focuses on essential features, so, models are less likely to overfit to noise in the data|\n",
    "|3|Enhanced Speed|Reduces data complexity to ensure faster training and inference times|\n",
    "|4|Model Generalization is high|Well-extracted features generalize better to unseen data|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Feature extraction is different for different data types. Illustrate.\n",
    "\n",
    "| **Data Type**       | **Feature Extraction Method**                                         |\n",
    "|---------------------|-----------------------------------------------------------------------|\n",
    "| **Categorical Data** | Encoding (One-Hot Encoding, Label Encoding, Frequency Encoding)      |\n",
    "| **Image Data**       | CNN-Based Feature Extraction, HOG, SIFT, Flattening (Preprocessing)  |\n",
    "| **Text Data**        | Text Representation (TF-IDF, Word Embeddings, Bag of Words)          |\n",
    "| **Numerical Data**   | Statistical Methods (PCA, LDA, Feature Scaling, Binning)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Elaborate about the need of FE in categorical data.\n",
    "\n",
    "* transforming non-numeric, discrete categories into meaningful numeric representations that ML algorithms can process effectively.\n",
    "* most ML models work with numerical input, encoding these categorical variables is critical for leveraging the predictive power of the model.\n",
    "* Non-numeric nature is undesirable as algorithms like linear regression or SVMs cannot process strings or categories directly.\n",
    "* Some categorical variables (e.g., ZIP codes or product IDs) may have many distinct values.\n",
    "* Improper encoding can introduce noise or cause the model to memorize data.\n",
    "* Proper encoding captures relationships between categories (ordinal, nominal) and retains meaningful structure.\n",
    "* Efficient encoding reduces the impact of high-cardinality categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give the steps involved in ML model building.\n",
    "\n",
    "1. Identify & seperate inputs and output from the whole dataset ie. D ---> X, y\n",
    "2. Split both input & output into train and test sets ie. X, y ---> X_train, X_test, y_train, y_test\n",
    "3. Apply data pre-processing on train set of input data ie. X_train ---> X_train_transformed\n",
    "4. Apply data pre-processing on test set of input data ie. X_test ---> X_test_transformed\n",
    "5. train the model on X_train_transformed & y_train\n",
    "6. Evaluate the model on X_train to get y_train_pred. Compare y_train & y_train pred to find train score (Mock Evaluation step)\n",
    "8. Do necessary corrections( change data preprocessing strategy or the algorithm) if train score is not satisfactory.\n",
    "8. evaluate the model using y_pred & y_test after testing on X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Data preprocessing comprises of _____ & _____.\n",
    "\n",
    "* data cleaning, feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Name the two things that are dependent on the output variable y.\n",
    "1. Choice of Algorithm\n",
    "2. Choice of Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the major procedures for cleaning text data?\n",
    "\n",
    "* Stop words removal\n",
    "* Removal of special characters\n",
    "* Stemming or Lemmatization\n",
    "* conversion to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Leakage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why is data preprocessing done after splitting data?\n",
    "\n",
    "* to prevent data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the need to prevent data leakage?\n",
    "\n",
    "**What is the need of testing of the ML model on test data ?**\n",
    "* it provides an unbiased evaluation of the model's generalization ability on unseen data\n",
    "* we need to ensure that the model performs well beyond the training set\n",
    "* it helps to identify issues like overfitting/ underfitting, which can compromise real-world performance. \n",
    "* Test data simulates real-world scenarios, offering insight into how the model will behave in practical applications. \n",
    "* By evaluating the model using various performance metrics, testing provides a quantitative measure of performance. \n",
    "* It also helps in comparing different models or hyperparameters objectively.\n",
    "* it builds confidence in deploying the model by ensuring it meets the desired performance standards.\n",
    "\n",
    "**Why data leakage is a villain ?**\n",
    "\n",
    "* test data is a simulation of real data.\n",
    "* if the model has already got a glimpse of it due to preprocessing before train-test split, the test results are useless.\n",
    "* testing won't serve its purpose.\n",
    "* DL leads to overly optimistic model performance by allowing information from the test set to influence the training process. \n",
    "* results in a model that appears to be accurate during evaluation but performs poorly in real-world scenarios. \n",
    "* undermines the integrity of the validation process, making it impossible to gauge the model’s true generalization ability. \n",
    "* DL can occur subtly, such as through shared preprocessing steps, making it harder to detect. Ultimately, \n",
    "* it compromises the model's reliability, rendering its predictions untrustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Demonstrate the problem of data leakage using an example.\n",
    "\n",
    "**Pre-processing before splitting :**\n",
    "\n",
    "1. **Raw dataset**\n",
    "\n",
    "||sentence|\n",
    "|--|----|\n",
    "|1|This is a big city.|\n",
    "|2|This is the biggest city,|\n",
    "|3|This is a $ warm city.|\n",
    "|4|This is the # warmest city.|\n",
    "|5|This @ is a cold city.|\n",
    "\n",
    "2. **After Preprocessing (Data cleaning)**\n",
    "\n",
    "||sentence|\n",
    "|--|----|\n",
    "|1|big city|\n",
    "|2|big city|\n",
    "|3|warm city|\n",
    "|4|warm city|\n",
    "|5|cold city|\n",
    "\n",
    "3. **After Preprocessing (FE using BoW)**\n",
    "\n",
    "||big|city|cold|warm|\n",
    "|--|----|----|----|---|\n",
    "|1|1|1|0|0|\n",
    "|2|1|1|0|0|\n",
    "|3|0|1|0|1|\n",
    "|4|0|1|0|1|\n",
    "|5|0|1|1|0|\n",
    "\n",
    "4. **Train-test-split**\n",
    "\n",
    "Train set :\n",
    "\n",
    "||big|city|cold|warm|\n",
    "|--|----|----|----|---|\n",
    "|1|1|1|0|0|\n",
    "|2|1|1|0|0|\n",
    "|3|0|1|0|1|\n",
    "\n",
    "Test set :\n",
    "\n",
    "||big|city|cold|warm|\n",
    "|--|----|----|----|---|\n",
    "|1|0|1|0|1|\n",
    "|2|0|1|1|0|\n",
    "\n",
    "The BoW encoding for the test set includes terms that the model already saw during training, leading to artificially inflated performance metrics and misleading results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing after splitting :**\n",
    "\n",
    "1. **Raw dataset**\n",
    "\n",
    "||sentence|\n",
    "|--|----|\n",
    "|1|This is a big city.|\n",
    "|2|This is the biggest city,|\n",
    "|3|This is a $ warm city.|\n",
    "|4|This is the # warmest city.|\n",
    "|5|This @ is a cold city.|\n",
    "\n",
    "2. **After Train test split**\n",
    "\n",
    "Train set :\n",
    "\n",
    "||sentence|\n",
    "|--|----|\n",
    "|1|This is a big city.|\n",
    "|2|This is the biggest city,|\n",
    "|3|This is a $ warm city.|\n",
    "\n",
    "Test set :\n",
    "\n",
    "||sentence|\n",
    "|--|----|\n",
    "|1|This is the # warmest city.|\n",
    "|2|This @ is a cold city.|\n",
    "\n",
    "3. **After pre-processing (data cleaning)**\n",
    "\n",
    "Train set :\n",
    "\n",
    "||sentence|\n",
    "|--|----|\n",
    "|1|big city|\n",
    "|2|big city|\n",
    "|3|warm city|\n",
    "\n",
    "Test set :\n",
    "\n",
    "||sentence|\n",
    "|---|----|\n",
    "|1|warm city|\n",
    "|2|cold city|\n",
    "\n",
    "3. **After Preprocessing (FE using BoW)**\n",
    "\n",
    "Train set :\n",
    "\n",
    "||big|city|warm|\n",
    "|--|----|----|--|\n",
    "|1|1|1|0|0|\n",
    "|2|1|1|0|0|\n",
    "|3|0|1|0|1|\n",
    "\n",
    "Test set :\n",
    "\n",
    "||big|city|warm\n",
    "|---|----|----|---|\n",
    "|1|0|1|1|\n",
    "|2|0|1|0|\n",
    "\n",
    "This approach accurately reflects the model’s real-world generalization ability, providing a more honest evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||Data cleaning|Feature Extraction|\n",
    "|--|----|-----|\n",
    "|Numerical data|numpy, pandas|sklearn|\n",
    "|Categorical data|numpy, pandas|sklearn|\n",
    "|Text data|nltk, spacy, fuzzywuzzy|sklearn(BOW & TF-IDF), gensim(Word2Vec & Glove & FastText), Keras/ Pytorch(RNN, LSTM, Transformers), |\n",
    "|Image data|PIL, Open CV|Keras, Pytorch|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is Hugging Face?\n",
    "\n",
    "* not just a library\n",
    "* a platform and ecosystem \n",
    "* encompasses various libraries, models, tools, and resources for NLP and ML\n",
    "* Hugging Face provides:\n",
    "   1. **Model Hub** : A repository for thousands of pre-trained models.\n",
    "   2. **Datasets Hub** : A collection of datasets for NLP, computer vision, and other ML domains.\n",
    "   3. **Spaces** : A platform to host, share, and deploy ML apps (using tools like Streamlit or Gradio).\n",
    "   4. **Inference API** : A service to run models in the cloud without worrying about infrastructure.\n",
    "   5. **Community** : A collaborative space for developers and researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is Transformers library?\n",
    "\n",
    "* Python-based open-source library developed by Hugging Face. \n",
    "* focuses on pre-trained transformer models, making it easier to integrate them into projects.\n",
    "\n",
    "|Feature|Description|\n",
    "|---|----|\n",
    "|Wide Model Support|Provides access to transformer-based architectures like BERT, GPT, RoBERTa, T5, and more.|\n",
    "|Pre-Trained Models|Ready-to-use models for tasks like text classification, question answering, translation, and text generation.|\n",
    "|Multi-Framework Support|Compatible with PyTorch, TensorFlow, and JAX.|\n",
    "|Ease of Fine-Tuning|Tools to fine-tune models on custom datasets with minimal code.|\n",
    "|Tokenizers Library|Highly efficient tokenization utilities optimized for speed and accuracy.|\n",
    "|Integration with Datasets and Accelerate|Seamless support for Hugging Face’s Datasets and Accelerate libraries.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Compare HF and transformers library w.r.t their use cases.\n",
    "||When to use?|\n",
    "|----|-----|\n",
    "|Hugging Face (Ecosystem)|If you need an end-to-end solution, from pre-trained models to deployment and hosting/For collaborative projects or sharing your models and apps with others.|\n",
    "|Transformers Library|If you're focusing on development and fine-tuning models programmatically./For implementing custom pipelines and deep integration into machine learning workflows.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Although tokenization is the 1st step in text data cleaning, it is not an isolated step. Justify.\n",
    "\n",
    "* breaks the text into words, enabling identification of stop-words\n",
    "* splits the text, making it easier to isolate special characters for removal\n",
    "* Lowercasing is often applied to tokens after splitting the text\n",
    "* tokenized words are required as input to apply stemming or lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Distinguish between stemming & lemmatization.\n",
    "\n",
    "**Stemming**: \n",
    " * process of reducing a word to its root or base form \n",
    " * by removing suffixes or prefixes\n",
    " * often without considering the actual meaning \n",
    " * e.g., \"running\" → \"run\" \n",
    " * faster processing speed\n",
    "\n",
    "**Lemmatization**: \n",
    " * process of reducing a word to its dictionary or base form (lemma) \n",
    " * considering its context and meaning \n",
    " * e.g., \"better\" → \"good\" \n",
    " * processing speed is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the 2 factors under trade-off while considering choice between stemming and lemmentaization?\n",
    "\n",
    "1. Processing speed\n",
    "2. Preservation of word meaning/ context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Construct a Python function named preprocess, that will do text data cleaning.\n",
    "\n",
    "```python\n",
    "# import required dependencies\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# download stopwords dataset if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def preprocess(raw_text, flag):\n",
    "\n",
    "    # remove special characters\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \"\", raw_text)\n",
    "\n",
    "    # change text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Stemming/ Lemmatization\n",
    "    if flag == \"stem\":\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else :\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    preprocessed_text = \" \".join(words)\n",
    "    words_in_processed_text = len(words)\n",
    "\n",
    "    return pd.Series([preprocessed_text, words_in_processed_text])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CV & Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What does train score value indicate?**\n",
    "\n",
    "* high value implies learning has happened\n",
    "* low value implies that no learning has happened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : For machine classification problem model, is there a cut-off that indicates desirable model?**\n",
    "\n",
    "* No, such cut-off\n",
    "* It totallyy depends on the problem statement and scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : (Algo, hyperparameter) ordered pairs for a single problem can be a huge number. How to decide the best combination?**\n",
    "\n",
    "* GridSearch CV\n",
    "* Randomised CV \n",
    "\n",
    "These features are present in sklearn. CV stands for cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WHAT ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is Cross validation?**\n",
    "\n",
    "* technique used to assess the performance and generalization ability of a machine learning algorithm\n",
    "* done by dividing the data into multiple parts and training/testing on different subsets\n",
    "* helps to avoid overfitting \n",
    "* ensures that the model performs well on unseen data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How is Cross validation done? Explain step-by-step.**\n",
    "\n",
    "1. Divide the data into K folds. Fix an algorithm\n",
    "2. Keep aside a fold for testing and compile rest of the K-1 folds into a training set and run the algo over it to get a model.\n",
    "3. test that model over the test fold.\n",
    "4. Repeat steps 2,3 for all K-folds.\n",
    "5. The average value of K performance scores gives an idea about the algorithm's generalization ability. It is a more reliable indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What are other types of CV?**\n",
    "\n",
    "1. **Stratified K-Fold CV :**\n",
    " - Whole data is divided into k-folds\n",
    " - the class distribution is preserved across every folds\n",
    " - useful when there is class imbalance in the dataset\n",
    "\n",
    "2. **Leave-One-Out-CV :**\n",
    " - Computationally very expensive\n",
    " - a datapoint is used as a test point and training is done on rest of the dataset\n",
    " - repeated for all datapoints in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Can cross validation be considered a thorough learning of the data?**\n",
    "\n",
    "* Although it appears to be a thorough learning process, CV is not actually that\n",
    "* During cross-validation, the algo is trained from scratch for each fold. It doesn’t retain knowledge from previous folds.\n",
    "* It is not thorough learning, but instead it is a thorough evaluation process\n",
    "* Its like thinking twice or thrice before fixing an algo for solving a problem\n",
    "\n",
    "CV does not contribute to the model's actual learning but rather evaluates how well an ML algorithm generalizes to unseen data. Each fold in CV serves only as a temporary training-validation split, and once CV is complete, those models are discarded. The final model is then trained separately on the full dataset. Hence, CV is a thorough evaluation technique rather than a learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Extend the 'student learning for exam' analogy of ML model building pipeline to include CV step.**\n",
    "\n",
    "CV is more like screening tests or entrance exams, where:\n",
    "\n",
    " - Multiple test setups are designed to evaluate the student (or algorithm).\n",
    " - Each test is independent; there’s no learning transfer between tests.\n",
    " - Once the student (algorithm) clears the screening (CV), the real training begins (training on the entire dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WHY ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is the prime purpose behind CV?**\n",
    "\n",
    "* Validation/ Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is that the CV is trying to evaluate?**\n",
    "\n",
    "- CV evaluates an algorithm, not a specific model instance\n",
    "- CV isn’t about improving or retaining knowledge in the model being trained\n",
    "- CV is about understanding :\n",
    "  * How well the algorithm (e.g., Random Forest, SVM) performs on the given data.\n",
    "  * How **consistent** the performance is across multiple data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Average of CV scores is a highly reliable indicator of what?**\n",
    "\n",
    "- of the algorithm’s generalization ability on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Why is it highly reliable?**\n",
    "\n",
    "1. **Multiple Validation Splits** \n",
    "  - CV evaluates the algorithm on multiple train-validation splits rather than a single train-test split, reducing bias from any specific data partition.\n",
    "2. **Reduced Variance** \n",
    "  - Averaging the scores across folds smooths out fluctuations caused by random data variations, leading to a more stable estimate of performance.\n",
    "3. **Better Approximation of Real-World Performance**\n",
    " - Since CV tests the algorithm on diverse subsets of data, the average score reflects how well the algorithm would perform on truly unseen data.\n",
    "4. **Prevents Overfitting to a Single Split**\n",
    "  - Without CV, a single train-test split might give an overoptimistic or overly pessimistic estimate, while CV provides a balanced evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : \"CV mitigates problem of overfitting and underfitting of a ML model (not ML algorithm)\". True or False. Justify.**\n",
    "\n",
    "**False.**  \n",
    "\n",
    "- CV helps evaluate and detect overfitting or underfitting\n",
    "- but it does **not** directly mitigate these issues in the final **ML model** \n",
    "- Instead, CV provides insight into how well an **ML algorithm** generalizes by testing it on multiple train-validation splits. \n",
    "- If overfitting or underfitting is observed, actions like changing the **algorithm, data preprocessing, or regularization techniques** must be taken to address it. \n",
    "- CV itself does not alter the model’s parameters or learning process—it only assesses an algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Doing CV is like thinking twice or thrice before fixing a ML algo for solving a problem. Justify.**\n",
    "\n",
    "- CV allows us to **evaluate** an ML algorithm on multiple data splits before finalizing it for the problem. \n",
    "- Just as thinking twice or thrice helps in making a well-informed decision, CV helps in assessing whether an algorithm generalizes well to unseen data, avoiding **hasty or biased conclusions** based on a single train-test split\n",
    "- provides a clearer picture of an algorithm’s **stability and consistency**, helping us decide whether it is the right choice for the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WHERE ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV is used in machine learning model evaluation, feature selection, and hyperparameter tuning to ensure robust performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is applied when data is limited, when avoiding overfitting is crucial, or when comparing models or hyperparameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How is CV used for hyperparameter tuning? Explain step-by-step.**\n",
    "\n",
    "1. **Initial Data Split**\n",
    "  * Typically, the dataset is initially split into a training set (e.g., 80%) & a testing set\n",
    "  * training set is used for CV ie. further divided into folds\n",
    "  * test set is also called as **holdout set** is held out and never seen by the model during training or CV\n",
    "\n",
    "2. **Folding the training Set**\n",
    "  * training dataset is divided into k equal-sized subsets (folds).\n",
    "  * One subset is used for validation, and the remaining k-1 subsets are used for training.\n",
    "\n",
    "3. **Training**\n",
    "  * (k-1) subsets form a single training set\n",
    "  * we get a single model upon running the algorithm on this training set\n",
    "  * this model is validated using the test subset\n",
    "\n",
    "4. **Repeat**\n",
    "  * This process is repeated k times, each time using a different fold for validation purpose.\n",
    "  * hence, cross-validation\n",
    "\n",
    "5. **Average Performance**\n",
    "  * In the end, I have k different models and their performance scores\n",
    "  * The final performance is computed as the average of the scores across all different training subsets\n",
    "  * this value gives idea about how a single hyperparameter setting performs on an average\n",
    "  * the same process is done for various hyperparameter settings\n",
    "\n",
    "6. **Comparison**\n",
    "  * after comparing average performances of various hyperparameter settings, the best hyperparameter setting is chosen\n",
    "\n",
    "7. **Actual training**\n",
    "  * the best hyperparameter setting is locked\n",
    "  * the algo is run under this setting over the entire train set\n",
    "  * a single final model is obtained which will be tested on the holdout set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : For each hyperparameter setting, what is the need of training over k different subsets & then computing average performance? Why not on a single training set? What is the need of this double work? Or in other words why cross validate for a single hyperparameter setting?**\n",
    "\n",
    "OR\n",
    "\n",
    "**Q : What is the need of cross validation over single simple validation?**\n",
    "\n",
    "**A :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) *What is the problem with a Single Training-Validation Split?*\n",
    " \n",
    "**Bias from Random Splitting :** \n",
    "  - single split may accidentally contain \"easy\" or \"hard\" examples in the training or test set\n",
    "  - will lead to overly optimistic or pessimistic performance evaluation\n",
    "  - model may perform well on this specific split but poorly on unseen data\n",
    "\n",
    "**Overfitting to a Particular Split :** \n",
    "  - The model might learn patterns specific to the given training set\n",
    "  - since you're testing on just one test set, you don't know if the model generalizes well across different subsets\n",
    "  - A good test score might give a false sense of confidence.\n",
    "\n",
    "**Data Imbalance Issues :**\n",
    "  - Important patterns might be underrepresented in the test set due to class imbalance or sampling biases\n",
    "  - will lead to misleading performance evaluation\n",
    "  - CV ensures all patterns are tested across different subsets\n",
    "\n",
    "(ii) *Why k-fold CV is done for each hyperparameter setting?*\n",
    "\n",
    "**Reducing variance in Performance Estimation :**\n",
    "  - training on k different subsets allows the model to be evaluated across diverse portions of the data\n",
    "  - will lead to a more reliable estimate of performance\n",
    "  - averaged score smooths out any fluctuations caused by randomness in a single split\n",
    "\n",
    "**Ensuring Generalization Ability :**\n",
    "  - By using different validation sets in each fold, we ensure the model performs well on all parts of the data, not just one specific subset\n",
    "  - helps in selecting hyperparameters that generalize well to unseen data\n",
    "\n",
    "**Efficient use of Data :**\n",
    "  - With CV, each data point gets to be in the validation set exactly once and in the training set k-1 times, maximizing the use of limited data.\n",
    "  - Training on the full dataset in different parts ensures no information is wasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Summarize the differences and similarities between 'CV with hyperparameter tuning' and 'CV without hyperparameter tuning' in a table.** \n",
    "\n",
    "\n",
    "| Aspect                              | CV Without Hyperparameter Tuning                          | CV With Hyperparameter Tuning                                |\n",
    "|-------------------------------------|----------------------------------------------------------|------------------------------------------------------------|\n",
    "| **Purpose**                         | Evaluate model performance and generalization ability.    | Find the best hyperparameter settings for the model.        |\n",
    "| **Hyperparameters**                 | Fixed throughout the process.                            | Multiple hyperparameter combinations are tested.            |\n",
    "| **Number of Models**                | k models (from k folds).                                 | k models per hyperparameter combination, leading to more models overall. |\n",
    "| **Performance Score**               | Average performance score across k folds.                | Average performance score across k folds for each hyperparameter combination. |\n",
    "| **Final Model**                     | Retrained on the full dataset with fixed hyperparameters. | Retrained on the full dataset using the best hyperparameters found during tuning. |\n",
    "| **Computational Cost**              | Relatively low.                                           | Higher due to multiple hyperparameter combinations being evaluated. |\n",
    "| **Focus**                           | Assess the model’s generalization ability.               | Optimize the model's performance through parameter adjustment. |\n",
    "| **Use Cases**                       | Simple evaluation tasks or when hyperparameters are predetermined. | When tuning hyperparameters to maximize model performance.  |\n",
    "| **Analogy (Student Example)**       | Multiple mock tests to revise concepts, final revision based on fixed study plan. | Mock tests + optimizing study plan for best performance before final revision. |\n",
    "| **Practical Application**           | Less commonly used, mainly for baseline evaluation.       | Widely used in practice for building high-performing models. |\n",
    "| **Risk of Data Leakage**            | Minimal, provided test data remains separate.             | Minimal, provided test data remains separate.               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **HOW ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How to implement simple cross validation to check performance of RandomForest classifier algorithm on a dataset?**\n",
    "\n",
    "```python\n",
    "# import dependencies\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# set up the algorithm apparatus\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# perform CV\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv = kf, scoring = 'accuracy')\n",
    "\n",
    "# average performance of the algo\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average CV Score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Which module of sklearn has the classes GridSearchCV & RandomizedSearchCV?**\n",
    "\n",
    "* model_selection module\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is the purpose of the verbose attribute of GridSearchCV object?**\n",
    "\n",
    "* It indicates whether or not to display the metadata( what is happening internally while running a grid search CV)\n",
    "* verbose = 0 means, no need to show metadata\n",
    "* verbose = 1 means, show very little essential metadata\n",
    "* verbose = 2 means, show the entire detailed metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How is GridSearchCV executed in Python using sklearn?**\n",
    "\n",
    "```python\n",
    "'''STEP 01 - Import dependencies'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "'''STEP 02 - Define model'''\n",
    "model = SVC()\n",
    "\n",
    "'''STEP 03 - Setup a parameter grid'''\n",
    "# parameter grid is a dictionary basically\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  \n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "'''STEP 04 - Initialize a GridSearchCV object'''\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "'''STEP 05 - Run the object over whole dataset'''\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate final model on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What about Randomised search CV?**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Define model \n",
    "model = SVC()\n",
    "\n",
    "# Define parameter distribution\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 10),  \n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'gamma': uniform(0.01, 1)\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with 10 iterations\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Is hyperparameter tuning the only method to rectify overfitting issues with a model? If not, what are the other remedies?**\n",
    "\n",
    "No,\n",
    "\n",
    "1. Get more data\n",
    "2. Data Augmentation\n",
    "3. Early stopping\n",
    "4. Dropouts for DL\n",
    "5. Feature engineering (Transformation, selection)\n",
    "6. Regularization\n",
    "7. Ensembles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Best Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve a problem, I have a list of algorithms and several hyperparameter settings for it. By doing several CVs, I get the optimal hyperparameter setting for each algorithm. Out of these best of each algos, how to choose one? How to finalize a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is training phase in ML model building pipeline?**\n",
    "\n",
    "$\\text{training dataset} \\to \\boxed{\\text{algorithm}} \\to \\text{model}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is training time?**\n",
    "\n",
    "- refers to the total duration required for an ML algorithm to learn from the training dataset. \n",
    "- includes the time taken for multiple iterations (epochs), parameter updates, and optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What are the implications provided by training time of a model?**\n",
    "\n",
    "1. **Scalability**\n",
    "\n",
    "Longer training times may not be practical for large datasets or real-time applications.\n",
    "\n",
    "2. **Computational Cost**\n",
    "\n",
    "More training time often means higher resource consumption (CPU, GPU, memory).\n",
    "\n",
    "3. **Model Performance**\n",
    "\n",
    "Longer training might improve performance, but excessive training can lead to overfitting.\n",
    "\n",
    "4. **Experimentation Efficiency**\n",
    "\n",
    "Faster training allows quick iterations to experiment with different hyperparameters and architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is the size of a ML model?**\n",
    "\n",
    "- Model size refers to the amount of storage required to save the trained model\n",
    "- typically measured in kilobytes (KB), megabytes (MB), or gigabytes (GB). \n",
    "- depends on the number of parameters, layers, and data precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What are the implications provided by the size of a ML model?**\n",
    "\n",
    "1. **Deployment Feasibility** - Large models may not be suitable for edge devices (mobile, IoT) due to storage and processing limitations.\n",
    "\n",
    "2. **Inference Speed** - Smaller models usually have faster inference times, making them ideal for real-time applications.\n",
    "\n",
    "3. **Memory Efficiency** - Large models consume more RAM and may require specialized hardware (e.g., GPUs, TPUs).\n",
    "\n",
    "4. **Transferability** - A compact model is easier to deploy over networks, especially in cloud-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is Prediction Phase in ML model building pipeline?**\n",
    "\n",
    "\n",
    "$\\text{input test dataset} \\to \\boxed{\\text{trained model}} \\to \\text{output predictions}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Define prediction time.**\n",
    "\n",
    "* also called inference time\n",
    "* time taken by a trained ML model to generate predictions on new, unseen data during the testing or deployment phase\n",
    "* Some models perform better when processing multiple inputs at once (batch inference), while others optimize for single-instance inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : While building real time applications, what kind of prediction time is ideal?**\n",
    "\n",
    "- In critical applications like fraud detection, self-driving cars, or healthcare diagnostics, low inference time is essential for real-time decision-making.\n",
    "- A delay in prediction could lead to severe consequences (e.g., an autonomous vehicle reacting too late).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Does prediction time affect scalability and UX?**\n",
    "\n",
    "Yes. Faster inference improves user experience in web applications (e.g., chatbots, recommendation systems).\n",
    "A slow model may struggle with high traffic and reduce scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What does prediction time taken by a model imply about the hardware requirements?**\n",
    "\n",
    "- High inference times indicate that the model may be too complex for edge devices or mobile applications.\n",
    "- Lighter models (e.g., quantized models or pruned networks) can help reduce latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What are the factors that affect the prediction time taken by a model?**\n",
    "\n",
    "1. **Model Complexity** - More layers and parameters increase computation time.\n",
    "2. **Hardware** - CPUs are generally slower than GPUs/TPUs for deep learning models.\n",
    "3. **Optimizations** - Techniques like model quantization, pruning, and distillation can reduce prediction time.\n",
    "4. **Batch Size** - Larger batches may reduce per-sample latency but increase overall computational demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :\n",
    "(i) KNN classifier\n",
    "(ii) Logistic Regressor\n",
    "(iii) Decision tree classifier**\n",
    "\n",
    "**Compare the above algorithms based on training time, prediction time and model size.**\n",
    "\n",
    "**(i) KNN**\n",
    " - Least training time $\\because$ it doesn't actually \"train\" a model; it just stores the dataset.\n",
    " - takes more inference time, since it must calculate the distance from the test point to all training points.\n",
    " - model size is large because it keeps the entire dataset in memory.\n",
    "\n",
    "**(ii) Logistic Regressor**\n",
    " - takes high training time as it spents time to learn patterns in the data and relationships between features\n",
    " - inference time is low $\\because$ it computes a weighted sum of input features and applies a sigmoid function.\n",
    " - model size is small since it only stores a few learned coefficients.\n",
    "\n",
    "**(iii) DT Classifier**\n",
    " - higher training time. It depends on how deep the tree grows and the splitting criteria (e.g., Gini, entropy).\n",
    " - lower inference time, since making a prediction just involves traversing the tree.\n",
    " - Can be small or large depending on depth and pruning; deep trees have more nodes, increasing size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : In Python, how to identify whether class or function from the import statement?**\n",
    "\n",
    "- Class always begins with capital letter\n",
    "- eg.\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "# here, 'Pipeline' is a class\n",
    "```\n",
    "\n",
    "- Function always begins with small letter\n",
    "- eg. \n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# here, 'train_test_split' is a function\n",
    "```\n",
    "\n",
    "- although this is not mandatory, it is the convention usually followed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
