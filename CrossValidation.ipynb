{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CV & Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What does train score value indicate?**\n",
    "\n",
    "* high value implies learning has happened\n",
    "* low value implies that no learning has happened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : For machine classification problem model, is there a cut-off that indicates desirable model?**\n",
    "\n",
    "* No, such cut-off\n",
    "* It totallyy depends on the problem statement and scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : (Algo, hyperparameter) ordered pairs for a single problem can be a huge number. How to decide the best combination?**\n",
    "\n",
    "* GridSearch CV\n",
    "* Randomised CV \n",
    "\n",
    "These features are present in sklearn. CV stands for cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WHAT ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is Cross validation?**\n",
    "\n",
    "* technique used to assess the performance and generalization ability of a machine learning algorithm\n",
    "* done by dividing the data into multiple parts and training/testing on different subsets\n",
    "* helps to avoid overfitting \n",
    "* ensures that the model performs well on unseen data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How is Cross validation done? Explain step-by-step.**\n",
    "\n",
    "1. Divide the data into K folds. Fix an algorithm\n",
    "2. Keep aside a fold for testing and compile rest of the K-1 folds into a training set and run the algo over it to get a model.\n",
    "3. test that model over the test fold.\n",
    "4. Repeat steps 2,3 for all K-folds.\n",
    "5. The average value of K performance scores gives an idea about the algorithm's generalization ability. It is a more reliable indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What are other types of CV?**\n",
    "\n",
    "1. **Stratified K-Fold CV :**\n",
    " - Whole data is divided into k-folds\n",
    " - the class distribution is preserved across every folds\n",
    " - useful when there is class imbalance in the dataset\n",
    "\n",
    "2. **Leave-One-Out-CV :**\n",
    " - Computationally very expensive\n",
    " - a datapoint is used as a test point and training is done on rest of the dataset\n",
    " - repeated for all datapoints in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Can cross validation be considered a thorough learning of the data?**\n",
    "\n",
    "* Although it appears to be a thorough learning process, CV is not actually that\n",
    "* During cross-validation, the algo is trained from scratch for each fold. It doesn’t retain knowledge from previous folds.\n",
    "* It is not thorough learning, but instead it is a thorough evaluation process\n",
    "* Its like thinking twice or thrice before fixing an algo for solving a problem\n",
    "\n",
    "CV does not contribute to the model's actual learning but rather evaluates how well an ML algorithm generalizes to unseen data. Each fold in CV serves only as a temporary training-validation split, and once CV is complete, those models are discarded. The final model is then trained separately on the full dataset. Hence, CV is a thorough evaluation technique rather than a learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Extend the 'student learning for exam' analogy of ML model building pipeline to include CV step.**\n",
    "\n",
    "CV is more like screening tests or entrance exams, where:\n",
    "\n",
    " - Multiple test setups are designed to evaluate the student (or algorithm).\n",
    " - Each test is independent; there’s no learning transfer between tests.\n",
    " - Once the student (algorithm) clears the screening (CV), the real training begins (training on the entire dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WHY ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is the prime purpose behind CV?**\n",
    "\n",
    "* Validation/ Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is that the CV is trying to evaluate?**\n",
    "\n",
    "- CV evaluates an algorithm, not a specific model instance\n",
    "- CV isn’t about improving or retaining knowledge in the model being trained\n",
    "- CV is about understanding :\n",
    "  * How well the algorithm (e.g., Random Forest, SVM) performs on the given data.\n",
    "  * How **consistent** the performance is across multiple data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Average of CV scores is a highly reliable indicator of what?**\n",
    "\n",
    "- of the algorithm’s generalization ability on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Why is it highly reliable?**\n",
    "\n",
    "1. **Multiple Validation Splits** \n",
    "  - CV evaluates the algorithm on multiple train-validation splits rather than a single train-test split, reducing bias from any specific data partition.\n",
    "2. **Reduced Variance** \n",
    "  - Averaging the scores across folds smooths out fluctuations caused by random data variations, leading to a more stable estimate of performance.\n",
    "3. **Better Approximation of Real-World Performance**\n",
    " - Since CV tests the algorithm on diverse subsets of data, the average score reflects how well the algorithm would perform on truly unseen data.\n",
    "4. **Prevents Overfitting to a Single Split**\n",
    "  - Without CV, a single train-test split might give an overoptimistic or overly pessimistic estimate, while CV provides a balanced evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : \"CV mitigates problem of overfitting and underfitting of a ML model (not ML algorithm)\". True or False. Justify.**\n",
    "\n",
    "**False.**  \n",
    "\n",
    "- CV helps evaluate and detect overfitting or underfitting\n",
    "- but it does **not** directly mitigate these issues in the final **ML model** \n",
    "- Instead, CV provides insight into how well an **ML algorithm** generalizes by testing it on multiple train-validation splits. \n",
    "- If overfitting or underfitting is observed, actions like changing the **algorithm, data preprocessing, or regularization techniques** must be taken to address it. \n",
    "- CV itself does not alter the model’s parameters or learning process—it only assesses an algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Doing CV is like thinking twice or thrice before fixing a ML algo for solving a problem. Justify.**\n",
    "\n",
    "- CV allows us to **evaluate** an ML algorithm on multiple data splits before finalizing it for the problem. \n",
    "- Just as thinking twice or thrice helps in making a well-informed decision, CV helps in assessing whether an algorithm generalizes well to unseen data, avoiding **hasty or biased conclusions** based on a single train-test split\n",
    "- provides a clearer picture of an algorithm’s **stability and consistency**, helping us decide whether it is the right choice for the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WHERE ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV is used in machine learning model evaluation, feature selection, and hyperparameter tuning to ensure robust performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is applied when data is limited, when avoiding overfitting is crucial, or when comparing models or hyperparameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How is CV used for hyperparameter tuning? Explain step-by-step.**\n",
    "\n",
    "1. **Initial Data Split**\n",
    "  * Typically, the dataset is initially split into a training set (e.g., 80%) & a testing set\n",
    "  * training set is used for CV ie. further divided into folds\n",
    "  * test set is also called as **holdout set** is held out and never seen by the model during training or CV\n",
    "\n",
    "2. **Folding the training Set**\n",
    "  * training dataset is divided into k equal-sized subsets (folds).\n",
    "  * One subset is used for validation, and the remaining k-1 subsets are used for training.\n",
    "\n",
    "3. **Training**\n",
    "  * (k-1) subsets form a single training set\n",
    "  * we get a single model upon running the algorithm on this training set\n",
    "  * this model is validated using the test subset\n",
    "\n",
    "4. **Repeat**\n",
    "  * This process is repeated k times, each time using a different fold for validation purpose.\n",
    "  * hence, cross-validation\n",
    "\n",
    "5. **Average Performance**\n",
    "  * In the end, I have k different models and their performance scores\n",
    "  * The final performance is computed as the average of the scores across all different training subsets\n",
    "  * this value gives idea about how a single hyperparameter setting performs on an average\n",
    "  * the same process is done for various hyperparameter settings\n",
    "\n",
    "6. **Comparison**\n",
    "  * after comparing average performances of various hyperparameter settings, the best hyperparameter setting is chosen\n",
    "\n",
    "7. **Actual training**\n",
    "  * the best hyperparameter setting is locked\n",
    "  * the algo is run under this setting over the entire train set\n",
    "  * a single final model is obtained which will be tested on the holdout set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : For each hyperparameter setting, what is the need of training over k different subsets & then computing average performance? Why not on a single training set? What is the need of this double work? Or in other words why cross validate for a single hyperparameter setting?**\n",
    "\n",
    "OR\n",
    "\n",
    "**Q : What is the need of cross validation over single simple validation?**\n",
    "\n",
    "**A :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) *What is the problem with a Single Training-Validation Split?*\n",
    " \n",
    "**Bias from Random Splitting :** \n",
    "  - single split may accidentally contain \"easy\" or \"hard\" examples in the training or test set\n",
    "  - will lead to overly optimistic or pessimistic performance evaluation\n",
    "  - model may perform well on this specific split but poorly on unseen data\n",
    "\n",
    "**Overfitting to a Particular Split :** \n",
    "  - The model might learn patterns specific to the given training set\n",
    "  - since you're testing on just one test set, you don't know if the model generalizes well across different subsets\n",
    "  - A good test score might give a false sense of confidence.\n",
    "\n",
    "**Data Imbalance Issues :**\n",
    "  - Important patterns might be underrepresented in the test set due to class imbalance or sampling biases\n",
    "  - will lead to misleading performance evaluation\n",
    "  - CV ensures all patterns are tested across different subsets\n",
    "\n",
    "(ii) *Why k-fold CV is done for each hyperparameter setting?*\n",
    "\n",
    "**Reducing variance in Performance Estimation :**\n",
    "  - training on k different subsets allows the model to be evaluated across diverse portions of the data\n",
    "  - will lead to a more reliable estimate of performance\n",
    "  - averaged score smooths out any fluctuations caused by randomness in a single split\n",
    "\n",
    "**Ensuring Generalization Ability :**\n",
    "  - By using different validation sets in each fold, we ensure the model performs well on all parts of the data, not just one specific subset\n",
    "  - helps in selecting hyperparameters that generalize well to unseen data\n",
    "\n",
    "**Efficient use of Data :**\n",
    "  - With CV, each data point gets to be in the validation set exactly once and in the training set k-1 times, maximizing the use of limited data.\n",
    "  - Training on the full dataset in different parts ensures no information is wasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Summarize the differences and similarities between 'CV with hyperparameter tuning' and 'CV without hyperparameter tuning' in a table.** \n",
    "\n",
    "\n",
    "| Aspect                              | CV Without Hyperparameter Tuning                          | CV With Hyperparameter Tuning                                |\n",
    "|-------------------------------------|----------------------------------------------------------|------------------------------------------------------------|\n",
    "| **Purpose**                         | Evaluate model performance and generalization ability.    | Find the best hyperparameter settings for the model.        |\n",
    "| **Hyperparameters**                 | Fixed throughout the process.                            | Multiple hyperparameter combinations are tested.            |\n",
    "| **Number of Models**                | k models (from k folds).                                 | k models per hyperparameter combination, leading to more models overall. |\n",
    "| **Performance Score**               | Average performance score across k folds.                | Average performance score across k folds for each hyperparameter combination. |\n",
    "| **Final Model**                     | Retrained on the full dataset with fixed hyperparameters. | Retrained on the full dataset using the best hyperparameters found during tuning. |\n",
    "| **Computational Cost**              | Relatively low.                                           | Higher due to multiple hyperparameter combinations being evaluated. |\n",
    "| **Focus**                           | Assess the model’s generalization ability.               | Optimize the model's performance through parameter adjustment. |\n",
    "| **Use Cases**                       | Simple evaluation tasks or when hyperparameters are predetermined. | When tuning hyperparameters to maximize model performance.  |\n",
    "| **Analogy (Student Example)**       | Multiple mock tests to revise concepts, final revision based on fixed study plan. | Mock tests + optimizing study plan for best performance before final revision. |\n",
    "| **Practical Application**           | Less commonly used, mainly for baseline evaluation.       | Widely used in practice for building high-performing models. |\n",
    "| **Risk of Data Leakage**            | Minimal, provided test data remains separate.             | Minimal, provided test data remains separate.               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **HOW ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How to implement simple cross validation to check performance of RandomForest classifier algorithm on a dataset?**\n",
    "\n",
    "```python\n",
    "# import dependencies\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# set up the algorithm apparatus\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# perform CV\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv = kf, scoring = 'accuracy')\n",
    "\n",
    "# average performance of the algo\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average CV Score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Which module of sklearn has the classes GridSearchCV & RandomizedSearchCV?**\n",
    "\n",
    "* model_selection module\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What is the purpose of the verbose attribute of GridSearchCV object?**\n",
    "\n",
    "* It indicates whether or not to display the metadata( what is happening internally while running a grid search CV)\n",
    "* verbose = 0 means, no need to show metadata\n",
    "* verbose = 1 means, show very little essential metadata\n",
    "* verbose = 2 means, show the entire detailed metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : How is GridSearchCV executed in Python using sklearn?**\n",
    "\n",
    "```python\n",
    "'''STEP 01 - Import dependencies'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "'''STEP 02 - Define model'''\n",
    "model = SVC()\n",
    "\n",
    "'''STEP 03 - Setup a parameter grid'''\n",
    "# parameter grid is a dictionary basically\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  \n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "'''STEP 04 - Initialize a GridSearchCV object'''\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "'''STEP 05 - Run the object over whole dataset'''\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate final model on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : What about Randomised search CV?**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Define model \n",
    "model = SVC()\n",
    "\n",
    "# Define parameter distribution\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 10),  \n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'gamma': uniform(0.01, 1)\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with 10 iterations\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Is hyperparameter tuning the only method to rectify overfitting issues with a model? If not, what are the other remedies?**\n",
    "\n",
    "No,\n",
    "\n",
    "1. Get more data\n",
    "2. Data Augmentation\n",
    "3. Early stopping\n",
    "4. Dropouts for DL\n",
    "5. Feature engineering (Transformation, selection)\n",
    "6. Regularization\n",
    "7. Ensembles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve a problem, I have a list of algorithms and several hyperparameter settings for it. By doing several CVs, I get the optimal hyperparameter setting for each algorithm. Out of these best of each algos, how to choose one? How to finalize a model?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
