{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ANNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are ANNs?\n",
    "\n",
    "* ANN = Artificial Neural Network\n",
    "* they represent a family of non-linear models\n",
    "* inspired by biological neural network\n",
    "* well structured and sufficiently large customized NNs can approximate any function one can think of\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** \"ANNs are flexible models\". Justify.\n",
    "\n",
    "Whether the data \n",
    "\n",
    "* has static patterns\n",
    "* contains multi-dimensional inputs or\n",
    "* is sequential in nature\n",
    "\n",
    "ANNs can be applied for modelling. Hence, they are dominant among most ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Describe the basic structure of an artificial neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/artificialneuron.png\" alt=\"image description\" width=500 height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Inputs $(x_1, x_2, \\ldots, x_n)$**:\n",
    "   - These are the signals or values fed into the neuron\n",
    "   - Each input represents a feature of the data that the neuron processes\n",
    "\n",
    "2. **Weights $(w_1, w_2, \\ldots, w_n)$**:\n",
    "   - Each input $x_i$ is associated with a weight $w_i$\n",
    "   - weight of a feature determines the importance or contribution of that feature to the neuron\n",
    "   - These weights are learned during training of the NN\n",
    "\n",
    "3. **Transfer Function (Σ)**:\n",
    "   - The transfer function computes the *net input* \n",
    "   - by summing the weighted inputs and adding a bias (if present)\n",
    "   - Mathematically, this is:\n",
    "     $$\\text{net}_j = \\sum_{i=1}^{n} w_i \\cdot x_i + b$$\n",
    "   - This **summation process gathers all inputs into a single value**, which will then be passed to the activation function\n",
    "\n",
    "4. **Activation Function $\\phi$**:\n",
    "   - The activation function \n",
    "      * takes the net input &\n",
    "      * transforms it \n",
    "      \n",
    "      determining the final output $o_j$ of the neuron \n",
    "\n",
    "5. **Threshold $\\theta_j$**:\n",
    "   - In some models, a threshold $\\theta_j$ is set, which the net input must exceed for the neuron to activate. \n",
    "   - If the net input is below the threshold, the output might be set to zero or a baseline value, making it a conditional activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Define transfer functions in the context of an artificial neuron.\n",
    "\n",
    "It is a linear combination of weighted inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give examples of commonly used activation functions.\n",
    "\n",
    "* sigmoid\n",
    "* ReLU\n",
    "* tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the role of activation function in an artificial neuron?\n",
    "\n",
    "* They introduce non-linearity, enabling the neuron to learn complex patterns\n",
    "* activation function's output defines whether and to what extent the neuron \"fires\" or activates, influencing the signal passed to the next layer in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the type of commonly used artificial neurons.\n",
    "\n",
    "| Neuron Name         | Brief Description                                                                                       |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------------|\n",
    "| Perceptron          | <ul><li>produces binary outputs</li><li>used for linearly separable problems</li><li> $\\phi \\ =$     step function</li>               |\n",
    "| Sigmoid Neuron      | <ul><li>$\\phi \\ =$ sigmoid</li><li>outputs a continuous value between 0 and 1</li></ul>                   |\n",
    "| ReLU Neuron         | <ul><li> $\\phi \\ =$ ReLU</li> <li>outputs zero for negative inputs and the input itself if positive </li></ul>|\n",
    "| Tanh Neuron         | <ul><li>$\\phi \\ =$ tanh</li> <li>outputting values between -1 and 1</li><li>helps center data</li></ul>              |\n",
    "| Softmax Neuron      | Outputs a probability distribution over multiple classes, commonly used in classification tasks.       |\n",
    "| Spiking Neuron      | Mimics biological neurons with time-based activations, used in spiking neural networks (SNNs).         |\n",
    "| RBF Neuron          | radial basis function (Gaussian) as the activation, effective for pattern recognition tasks.    |\n",
    "| LSTM Neuron         | Used in Long Short-Term Memory networks, designed to handle sequential data and retain long-term memory.|\n",
    "| GRU Neuron          | Used in Gated Recurrent Units, a simpler alternative to LSTMs for processing sequential data.          |\n",
    "| Convolutional Neuron| Used in CNNs, applying convolution operations to detect spatial features in images.                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the problem that bothers learning process of multiple perceptrons connected to each other?\n",
    "\n",
    "* simple perceptron algorithm cannot be extended to these\n",
    "* simple gradient based optimization is not applicable to these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why so ?\n",
    "\n",
    "* Derivative of the step function vanishes almost everywhere, except at the origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Suggest methods to solve these problems.\n",
    "\n",
    "* replace the step function with smooth non-linear functions which are differentiable\n",
    "* use back propagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give smooth approximations to the step function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Define a layer in an ANN.\n",
    "\n",
    "* Layer refers to a group of multiple neurons that work together as a single unit\n",
    "* However, neurons in a layer are not interconnected with each other\n",
    "* neurons in different layers are interconnected according to the network architechture\n",
    "* mathematically, a layer = an intermediate vector in computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Backpropagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is meant by module in a neural network?\n",
    "\n",
    "* it is the input-function-output segment in a neural network\n",
    "* any neural network is composed of several such modules\n",
    "* $$x \\to \\boxed{\\ y \\ = \\ f(W\\cdot  x + b)} \\to y$$\n",
    "* W denotes the weight vector of a layer, b denotes the bias factor of the layer and f is the non-linear activation function of the layer \n",
    "* In a neural network, each layer/module typically consists of a linear transformation (using weights and biases) followed by a non-linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the role of activation function in a layer?\n",
    "\n",
    "* The activation function introduces non-linearity, enabling the network to learn complex mappings beyond just linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Given an objective function Q(.) of the neural network, what is the error signal of any module?\n",
    "\n",
    "* it is denoted as $e$\n",
    "* it is the partial differential of the objective function w.r.t the output of the module\n",
    "* $$e = \\frac{\\partial(Q)}{\\partial(y)}$$\n",
    "* This error signal e represents how much the output y affects the overall loss Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the use of error signal?\n",
    "\n",
    "* it enables the calculation of gradient of Q w.r.t all learnable parameters of the network\n",
    "* by means of a simple chain rule, we have :\n",
    "$$\\frac{\\partial(Q)}{\\partial(W)} = \\frac{\\partial(Q)}{\\partial(y)}\\frac{\\partial(y)}{\\partial(W)} = e\\frac{\\partial(f(W\\cdot x+b))}{\\partial(W)} = e f'(W\\cdot x + b) x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How are the parameters of a neural network learned ?\n",
    "\n",
    "* loss function or objective function of the network is designed as a function of the network parameters\n",
    "* by optimizing this objective function w.r.t the parameters, the network learns the value of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is AD ?\n",
    "\n",
    "* AD stands for Automatic Differentiation\n",
    "* it is a technique that is guaranteed to compute the gradient( of the objective function w.r.t network parameters ) in most efficient way for any NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How does AD work?\n",
    "\n",
    "1. $$\\text{Neural Network} = \\boxed{\\text{Composition of simpler functions, each of which is a network module}}$$\n",
    "2. AD technique passes some key messages (error signal) along the neural network\n",
    "3. During this passage, all gradients can be computed locally, within the scope of each of the modules using these messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the working modes of the AD technique.\n",
    "\n",
    "1. Forward accumulation mode\n",
    "2. Reverse accumulation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Reverse accumulation mode of AD is enabled via. ___ algorithm.\n",
    "\n",
    "* Error Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to generate error signals for all modules in a NN in RAM of AD ?\n",
    "\n",
    "* We have to propagate th error signal along the neural netwrok in a certain way\n",
    "* propagation should be from the output end of a module to the input end of the module, so that it can be used as the error signal for the previous module\n",
    "* While we travel along a neural network in the reverse direction, we observe that :\n",
    "$$\\boxed{\\text{ Input of the current module }} = \\boxed{\\text{ Output of the previous module}}$$\n",
    "* the propagation of the error signal is continued till the 1st module of the network is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Consider this module of the NN given below.\n",
    "\n",
    "$$\\text{input }x \\to \\boxed{\\ y \\ = \\ f_W(x)} \\to \\text{output }y$$\n",
    "\n",
    "Represent mathematically the error backpropagation for this module if e is its error signal.\n",
    "\n",
    "**A :**\n",
    "$$\\frac{\\partial(Q)}{\\partial(x)} = \\frac{\\partial(Q)}{\\partial(y)}\\frac{\\partial(y)}{\\partial(x)} = e\\times \\frac{\\partial(f_W(x))}{\\partial(x)} $$\n",
    "\n",
    "* Here, $e$ is the error signal of the current module using which we get $\\frac{\\partial(Q)}{\\partial(x)}$ viz. the error signal of the previous module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Let's discuss the more general and realistic case where x and y are vectors. Say $x\\in R^m$ & $y\\in R^n$.\n",
    "\n",
    "**A :** Let $x = (x_1,x_2,\\dots, x_m)$, $y = (y_1, y_2, \\dots, y_n)$ be the input and output vectors to a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give the pseudocode to understand how error backpropagation helps the model to learn its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**ERROR BACKPROPAGATION PSUEDOCODE**\n",
    "\n",
    "0. Initialize network parameters (weights and biases) randomly\n",
    "\n",
    "**Forward pass**\n",
    "1. Take a training sample (x, target) & input x to the network\n",
    "2. For each layer in the network:\n",
    "    - Calculate the output of each neuron using its weights and biases\n",
    "    - Store the output for use in backpropagation\n",
    "\n",
    "**Compute the Error at the Output Layer**\n",
    "\n",
    "3. Calculate the error at the output layer using the loss function:\n",
    "  $$\\boxed{\\text{error} = \\text{Loss(output, target)}}$$\n",
    "\n",
    "**Backward Pass (Backpropagation)**\n",
    "\n",
    "4. Calculate the error gradient for the output layer:\n",
    "    - Compute the error signal for each neuron in the output layer\n",
    "    - $$\\boxed{\\text{Error signal }(e) = \\text{ derivative of Loss with respect to output layer activations}}$$\n",
    "5. Propagate the error backward through each layer, starting from the output layer and moving backwards by :\n",
    "    - Calculate the gradient of the error with respect to each weight in the layer\n",
    "    - $$\\boxed{\\frac{\\partial(Q)}{\\partial(W)} = \\text{error signal * derivative of activation function * previous layer output}}$$\n",
    "    - Store the gradient for each weight and bias in this layer\n",
    "\n",
    "**Update Parameters**\n",
    "\n",
    "6. For each layer, for each weight W in that layer and bias b of the layer:\n",
    "    - Update W using gradient descent:\n",
    "    $$W = W - \\eta  \\frac{\\partial(Q)}{\\partial(W)}$$\n",
    "                \n",
    "    $$b = b - \\eta \\frac{\\partial(Q)}{\\partial(b)}$$\n",
    "7. Repeat steps 1-6 for all training samples to complete one training epoch\n",
    "\n",
    "**Optionally track and print loss to monitor training progress**\n",
    "\n",
    "8. Calculate and print total loss for the epoch if needed\n",
    "9. Repeat until convergence or maximum number of epochs reached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Deep neural networks are so powerful models. Although the theory behind such networks were found in the mid 10th century, they became popular only after 2010. Why did it take so long for them to be popular?\n",
    "\n",
    "* training deep nets can be hard\n",
    "* training is done using the backpropagation algorithm\n",
    "* this method can lead to problems like :\n",
    "   - vanishing gradient\n",
    "   - exploding gradient\n",
    "* they prevent the smooth & successful training of the model\n",
    "* till 2006, there was no way to accurately train deep ANN due to vanishing gradient problem\n",
    "* till 2006, the deep NNs performed poorly in comparison with shallow NNs and other ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Breakthrough Research Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** RBMs stands for ____.\n",
    "\n",
    "* Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Which research paper introduced the concept of RBM ?\n",
    "\n",
    "* **Paper :** \"Training Products of Experts by Minimizing Contrastive Divergence\"\n",
    "* **Author :** Geoffrey E. Hinton\n",
    "* **Year :** 2002\n",
    "* **Brief :** \n",
    "   - describes the RBM \n",
    "   - introduces the contrastive divergence algorithm, a fast and efficient way to train RBMs\n",
    "   - foundation for many deep learning architectures \n",
    "   - catalyzed the resurgence of neural networks\n",
    "   - led to the development of deep belief networks (DBNs) and other deep learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Vanishing Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Define vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the causes of VG problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the consequences of VG problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain the math behind VG problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the solutions that will help to mitigate the VG problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Exploding Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Define exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the causes of EG problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the consequences of EG problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain the math behind EG problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** **Q :** What are the solutions that will help to mitigate the EG problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some Clarifications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* any single layer is composed of many neurons\n",
    "* each neuron has different parametes and (or) activation function\n",
    "* each neuron independently generates a single scalar output\n",
    "* all the scalar outputs produced by all neurons in a layer are organized as the components of a vector\n",
    "* this vector that comprises scalar outputs from the previous layer's neuron will act as the input vector to each and every neuron in the next layer\n",
    "* each neuron in the next layer will process this vector independently and generate its own output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are RNNs?\n",
    "\n",
    "* a type of neural network designed to handle sequential data\n",
    "* processes inputs one step at a time\n",
    "* each step's output is influenced by previous steps\n",
    "* allows it to retain information from earlier in the sequence\n",
    "* useful for tasks like \n",
    "    * language processing \n",
    "    * time series analysis \n",
    "    \n",
    "However, RNNs struggle with long-term dependencies due to issues like the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the drawbacks of traditional ANNs that led to the discovery of RNNs.\n",
    "\n",
    "1. Traditional neural networks are not able to deal with variable length inputs\n",
    "2. They also do not consider the sequential order of the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **TIME DELAYED FEEDBACK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** There are several ways to connect 2 layers of a neural network. What is time-delayed feedback connection?\n",
    "\n",
    "* it is a simple strategy to introduce memory mechanism into NN\n",
    "* done by adding some time delayed feedback paths to the NN\n",
    "* TDFPs is used to send the status of a layer **y** back to a previous layer as a part of its next input\n",
    "* NNs containing such FPs are called RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the components of a time-delayed feedback connection?\n",
    "\n",
    "1. **Input Signal**: \n",
    "   * original data or sequence step that is fed into the network at each time step \n",
    "   * e.g., a word in a sentence or a data point in a time series\n",
    "\n",
    "2. **Recurrent Connection**: \n",
    "   * connection through which the output from one time step (or the hidden state) is passed as input to the next time step\n",
    "   * This creates a \"loop\" that allows the network to retain information from previous steps\n",
    "\n",
    "3. **Hidden State**:\n",
    "   * internal representation (usually a vector) that holds information about previous steps, updated at each time step to reflect new input and past context\n",
    "   * critical to maintaining memory over a sequence\n",
    "\n",
    "4. **Output Signal**: \n",
    "   * final processed **output from the current time step**, which can be used for predictions or further layers\n",
    "   * and which also feeds back into the next step as part of the recurrent connection\n",
    "\n",
    "In advanced RNNs like LSTMs and GRUs, additional **gating mechanisms** (like forget, input, and output gates) are included to control how much information from past steps is retained or discarded in the hidden state, further refining the feedback loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give an analogy to better understand a feedback path.\n",
    "\n",
    "Analogy : a **teacher giving a series of lessons to a student**, where each lesson builds on the last to deepen the student’s understanding of a topic\n",
    "\n",
    "\n",
    "|Component|Analogy|\n",
    "|---|----|\n",
    "|Input signal|today's lesson, the new lesson material or note shared by the teacher|\n",
    "|Recurrent connection|using the understanding of the past lesson for understanding today's lesson by the student|\n",
    "|Hidden state|student's notebook or memory that is getting updated after each class|\n",
    "|Output|student's understanding after today's class viz. derived from the notes given by the teacher|\n",
    "|gates|review and focus areas|\n",
    "\n",
    "\n",
    "1. Every day, the teacher introduces a new concept or lesson notes\n",
    "\n",
    "2. As the teacher presents each lesson, the student uses his/her understanding of the previous lesson for better understanding that lesson.\n",
    "\n",
    "3. The student keeps a notebook and has a mental understanding of what’s been learned so far.\n",
    "\n",
    "4. After each lesson, the student’s current level of understanding is like the output. This comprehension is a result of integrating today’s new lesson (input) with all the prior knowledge (hidden state) and influences their performance in that day’s classwork or questions.\n",
    "\n",
    "5. Sometimes, the teacher reviews certain critical concepts, reinforces essential ideas, or lets go of less important points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/timedelayfeedback.jpeg\" alt=\"image description\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** In the above diagram, \n",
    "\n",
    "* $y_t$ = output of the network at the current time step t\n",
    "* $x$ = current input\n",
    "* This is the result of processing the current input x along with the feedback from the previous output $y_{t-1}$ \n",
    "\n",
    "​Essentially, $y_{t-1}$ provides the \"memory\" of what was produced at the last time step and is used as context for generating the current output $y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** In the above diagram, ___ is a delayed version of $y_t$?\n",
    "\n",
    "* $y_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why is this feedback loop called as **time-delayed**?\n",
    "\n",
    "* term **\"delayed version of $y_t$\"** means that the output $y_t$, which is produced at the current time step $t$, is not used immediately in the feedback loop\n",
    "* it is **held back (or \"delayed\") by one time step** and only fed back into the model during the next time instance $t+1$\n",
    "\n",
    "1. **At time $t$**, the model produces an output $y_t$ based on the current input $x$ and the feedback from the previous output $y_{t-1}$\n",
    "2. This output $y_t$ is stored and **\"delayed\"** for one time step\n",
    "3. **At time $t+1$**, the output $y_t$ (now considered as $y_{t-1}$ from the model's perspective is fed back as part of the input to help generate the new output  $y_{t+1}$.\n",
    "\n",
    "The \"delay\" simply means that **each output is used as feedback in the following step, not immediately in the same step**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give an analogy to represent this delay mechanism.\n",
    "\n",
    "\n",
    "Imagine you’re writing a diary every night about what you learned each day:\n",
    "\n",
    "1. On **Monday**, you write down what you learned that day. That’s your \"output\" for Monday.\n",
    "2. On **Tuesday**, you review Monday’s notes (the delayed version of what you wrote on Monday) to help you understand Tuesday's new information better.\n",
    "3. On **Wednesday**, you look back at Tuesday’s notes, and so on.\n",
    "\n",
    "Each day's understanding (output) becomes useful **only on the following day**; that’s the \"delay\". Similarly, in a neural network, $y_t$ is delayed by one time step and used in the next cycle as $y_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **TAPPED DELAY LINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is tapped delay line connection?\n",
    "\n",
    "* another strategy to introduce memory mechanism to NN\n",
    "* no recurrent feedback is used in this strategy\n",
    "* a TDL is a number of synchronized memory units aligned in a line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the meaning of a delay line in this context ?\n",
    "\n",
    "* term \"tapped delay line\" comes from signal processing and electronics\n",
    "* it originally referred to a physical line or medium (like a wire or a series of circuits) that could store and delay a signal for a certain amount of time\n",
    "* Delay Line\" refers to a series of storage elements that hold onto a signal temporarily, creating a delay\n",
    "* its like a sequence of “buckets” that each hold the signal for one time step, passing it along to the next bucket after each step\n",
    "* In neural networks, this \"line\" delays each input over a few time steps, creating a short-term memory of past inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Tapped means?\n",
    "\n",
    "* tap = to make use of a source of energy, knowledge, etc. that already exists\n",
    "* tap is a point along the delay line where you can access or \"tap into\" the stored signal\n",
    "* Imagine placing sensors along a water pipeline that capture the flow at various points\n",
    "* each tap represents a specific point in the past (like 1 step ago, 2 steps ago, etc.)\n",
    "* in a tapped delay line, multiple points (taps) are set along the delay sequence, allowing the network to access different past states at each tap\n",
    "* In neural networks, these taps provide the model with access to several past inputs simultaneously, enriching the current decision with a range of historical information\n",
    "\n",
    "In short, the term “tapped delay line” reflects the idea of having multiple \"taps\" along a delayed sequence to capture different points in time, creating a memory window of past inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain the function of a tapped delay line using the diagram.\n",
    "\n",
    "<img src=\"Images/tappeddelay.jpeg\" alt=\"image description\" width=850 height=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $y_t$ represents the input or the value fed into the layer $Y$ at the current time step $t$\n",
    "* $z^{-1}$ represents the delay of input by one-time step\n",
    "* $a_0, a_1,\\dots$ represent the weights of the delayed versions of previous inputs in the current output\n",
    "* $\\hat{z}_t$ is output of the tapped delay line, which is a combination of the current and past values of y after applying the respective weights\n",
    "\n",
    "Using this setup, the tapped delay line output $\\hat{z}_t$ at time $t$ is a weighted sum of y at the current and previous time steps. Mathematically, this can be expressed as:\n",
    "\n",
    "$$\\hat{z}_t = a_0y_{t}+a_1y_{t-1}+a_2y_{t-2}+...a_{L-1}y_{t-(L-1)}$$\n",
    "\n",
    "where $L$ is the length of delay line. This allows the tapped delay line to maintain a form of temporal memory over multiple time steps, where the influence of each past value can be controlled by its associated weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Describe the whole concept for thorough understanding.\n",
    "\n",
    "1. The memory units in the diagram store the values of layer y at previous time instances as the length of delay line ie. $y_t, y_{t-1},\\dots$\n",
    "2. At next time instant $t+1$, all values saved in the memory units are shifted right by 1\n",
    "3. At any time instance t, all the stores values are linearly combined through some learnable parameters to genrate a new layer of outputs denoted as $\\hat{z}_t$\n",
    "4. the learnable parameter $a_i$ can be chosen as :\n",
    "    * a scalar\n",
    "    * a vector\n",
    "    * a matrix\n",
    "5. $y_t, y_{t-1}, \\dots$ are vectors (vector representation of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** An important aspect of tapped delay line is that the generated vector $\\hat{z}_t$ will be  sent to the next layer. It is basically a non-recurrent feed forward structure that does not introduce any cycle into the neural network. Justify.\n",
    "\n",
    "* the **previous inputs** are only stored in the memory, not the **previous outputs**\n",
    "* inputs always remain at the input end only, so no need to feed anything back\n",
    "* output is fed into next layer not the previous layer, hence feed forward structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Modify the previous analogy to explain the tapped delay line connection.\n",
    "\n",
    "|Component|Analogy|\n",
    "|----|----|\n",
    "|Input|teacher notes|\n",
    "|output|students understanding|\n",
    "|delay line|file of paper notes(oldest paper is removed and newest note paper is added)|\n",
    "\n",
    "1. Teacher notes from previous days are used for learning the today's class\n",
    "2. The student does not use what his/her understanding or test feedback from previous classes for understanding today's class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Suppose the length of delay line is k & the number of tokens to process the entire corpus by the TDL is N. Then after training, how many model parameters will be obtained?\n",
    "\n",
    "* In a tapped delay line model, each tap position has an associated weight, meaning you have k weights that correspond to each tap position in the delay line\n",
    "* These weights are applied across all training examples, which means that for each new set of taps encountered during training, the model uses the same weights\n",
    "* During training, as each set of k taps are processed the weights are adjusted slightly based on the loss or error signal.\n",
    "* However, these weights are updated in such a way that they generalize across the entire dataset\n",
    "* After processing one set of taps, the weights are incrementally modified, but they remain the same weights for the next set of taps\n",
    "* This process repeats for each of the N training examples, continuously refining this single set of k weights\n",
    "* At the end of training, the model retains only one set of k weights. These weights represent the learned importance of each tap (delay) across the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why only a single setof weights?\n",
    "\n",
    "* The tapped delay line model aims to learn a generalizable pattern in the data rather than memorizing specific weights for each tap set.\n",
    "* By having a single set of k weights, the model can apply the learned importance of each time delay position consistently across different input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **ATTENTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Attention connection is a modified version of tapped delay line. Justify.\n",
    "\n",
    "* the coefficients $\\{a_0,a_1, \\dots\\}$ are all learnable parameters in tapped delay line\n",
    "* in a TDL, once the parameters are learned, they remain constant as network parameters\n",
    "* in AM, the aim is to dynamically adjust these coefficients to select the most prominent feautres from all saved info based on :\n",
    "   1. the current input token to be processed &\n",
    "   2. present internal status of the network\n",
    "* Unlike TDF with a fixed window, attention doesn’t impose a strict cutoff on the number of important tokens. Instead it saves previous tokens at any instant of time\n",
    "* Unlike TDLs, where some inputs are discarded after a set number of delays, attention mechanisms retain and can weigh all tokens, which is especially valuable for capturing long-term dependencies \n",
    "* Instead of a limited window, attention allows each token to be informed by the entire sequence, so the model can dynamically focus on whichever parts of the sequence are most relevant, regardless of distance\n",
    "* the model has access to all previous (and even current) inputs at any point in time. Each input token (or word) can \"attend\" to any other token in the sequence, regardless of how far back it appeared.\n",
    "\n",
    "In attention mechanism, all previous inputs are available and can be “stored” at any instance of time — a stark contrast to the fixed memory limitation in a TDL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** At any instance of time, all previous inputs are avaliable. What about the weights of these tokens for processing the current token?\n",
    "\n",
    "* while all previous inputs are technically accessible, the model assigns different levels of importance (or weights) to each stored input when processing the current input\n",
    "* each stored input from the sequence is given a different weight based on its relevance to the current input. These weights are called attention scores\n",
    "* While technically all tokens are considered, only a subset may receive significant weight and, therefore, contribute meaningfully to the understanding of the current input.\n",
    "* Inputs that are less relevant to the current input will receive lower weights, and thus, their impact will be minimal or even negligible\n",
    "* All tokens are \"eligible\" to be attended to, but only those with higher weights (attention scores) will actually influence the processing of the current token.\n",
    "* self-attention mechanisms like in Transformers use techniques like **softmax normalization** to ensure all tokens contribute but with different weights. This allows the model to focus on the few most relevant tokens while still having access to the full context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain the working of a single attention layer using a diagram.\n",
    "\n",
    "<img src=\"Images/attention.jpeg\" alt=\"image description\" width=900 height=250>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Input Sequence**\n",
    "* The top row represents the sequence of inputs y at different time steps; $y_t, y_{t-1},\\dots$ \n",
    "* each value in this sequence represents a stored or past value that is relevant for generating the output at time t\n",
    "* seq represents a sentence (sequence of words as embeddings)\n",
    "* each $y_{t-i}$ will be an embedding of a word in the sentence being processed by the layer\n",
    "\n",
    "\n",
    "2. **Delayed signals**\n",
    "* the signal $y_t$ is passed through a series of delay units (denoted as $z^{-1}$)  which shift the values back in time, enabling the layer to access historical data at each time step, maintaining multiple past values in memory\n",
    "\n",
    "3. **Attention weights**\n",
    "* While the layer is processing any sentence, at any instance of time t, it computes weights ${a_0,a_1,\\dots}$ for the previous tokens in that sentence based on the current token or context\n",
    "\n",
    "4. **Output**\n",
    "\n",
    "* $$\\hat{z}_t = a_0(t)y_t + a_1(t)y_{t-1}+...$$\n",
    "* The weighted sum operation combines the historical values using their respective coefficients\n",
    "* The sum is then passed to the next layer or used for further processing, providing a contextually weighted output at each time step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the significance of the dynamic nature of the attention weights ?\n",
    "\n",
    "* dynamic nature means we compute the attention weights at each instance while processing a sentence.\n",
    "* these weights reflect the relevance or importance of each historical input $y_{t-i}$ at the current time t\n",
    "* Unlike fixed weights, these coefficients can change for every time step, allowing the model to focus on different parts of the sequence as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What happens after a sentence gets processed by the attention layer or TDL? Point out the difference.\n",
    "\n",
    "**Attention**: Each sentence is processed in isolation, with all previous tokens within the sentence available through dynamically computed attention weights. It can capture long-range dependencies within a sentence.\n",
    "\n",
    "**Tapped Delay Line**: Each sentence is also processed in isolation, but only with a fixed number of recent tokens available (determined by the taps). It only captures short-range dependencies and uses static weights, which makes it less adaptive and unable to retain long-term context.\n",
    "\n",
    "|Aspect|Attention layer|TDL|\n",
    "|---|---|----|\n",
    "|Handling sequential data|processes each sequence independently|same|\n",
    "|Memory & dependency range|long|short|\n",
    "|weights|dynamic & content based|fixed|\n",
    "|cross sentence info|does not carry info across sentences unless a document level task|same|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is attention function ?\n",
    "\n",
    "* denoted as \"$g$\"\n",
    "* inputs : Query vector(Q), Key vector(K), Value vector(V)\n",
    "* outputs : $softmax\\left(\\frac{QK^t}{\\sqrt{d_k}}\\right)V$\n",
    "* purpose : to compute attention scores\n",
    "* time dependency : The attention weights are not fixed; they are recomputed at each time step based on the current query and the full set of keys. This means that the attention weights change depending on where the model is in the sequence and what information is currently most relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is query vector? What is its significance?\n",
    "\n",
    "* it represents the current input token’s perspective\n",
    "* it captures what the current token seeks from other tokens to enhance its contextual understanding\n",
    "* It interacts with key vectors (representing other tokens) to compute attention scores, which determine the relevance of each token to the current input. \n",
    "* The significance of the query vector lies in its role in dynamically focusing on relevant information, enabling context-aware representations in tasks like language understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is query vector computed? What is it's dimension?\n",
    "\n",
    "1. From a corpus, its embedding matrix is calculated, say X.\n",
    " $$X_{N\\times d} \\ \\begin{cases}N - \\text{number of unique words in the corpus}\\\\ d - \\text{dimension of the word embeddings}\\end{cases}$$\n",
    "2. Then a matrix of parameters $W_Q$ comes into picture. It will act as a linear transformation that transforms the word embedding into a query vector with $d_k$ components\n",
    "$${W_Q}_{d\\times d_k}\\begin{cases}d - \\text{dimension of the word embeddings}\\\\ d_k - \\text{dimension of the query vectors}\\end{cases}$$\n",
    "Using $W_Q$, the query vector corresponding to the current token (an embedding vector) can be calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is key vector? What is its signifcance?\n",
    "\n",
    "* it represents a feature or characteristic of an input element\n",
    "* it serves as a point of reference for determining relevance during the attention calculation\n",
    "* It helps to identify which values (or value vectors) should be attended to, based on their similarity to the query vector. \n",
    "* Essentially, the key vector signifies the importance of an input element or token in relation to a specific query in the sequence processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is query vector computed? What is it's dimension?\n",
    "\n",
    "1. From a corpus, its embedding matrix is calculated, say X.\n",
    " $$X_{N\\times d} \\ \\begin{cases}N - \\text{number of unique words in the corpus}\\\\ d - \\text{dimension of the word embeddings}\\end{cases}$$\n",
    "2. Then a matrix of parameters $W_K$ comes into picture. It will act as a linear transformation that transforms any word embedding into a key vector with $d_k$ components\n",
    "$$dim(W_K) = dim(W_Q) = d\\times d_k$$\n",
    "Using $W_K$, the key vector corresponding to the current token (an embedding vector) can be calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is value vector? What is its significance?\n",
    "\n",
    "* it contains the actual information or data associated with an input element in an attention mechanism\n",
    "* It is used to produce the output of the attention process by weighing the importance of each value based on the similarity between the query vector and the key vectors\n",
    "* The significance of the value vector lies in its role in delivering the relevant information that contributes to the final output, depending on how much attention is given to each input\n",
    "* value vector of a word in the context of attention mechanisms can be considered similar to its embedding\n",
    "* However, in the attention mechanism, the value vector specifically **serves to convey the information that is weighted according to the attention scores**, while embeddings can be used in various contexts beyond attention, such as in input representations for neural networks\n",
    "* Both represent the word in a continuous vector space, capturing semantic information and relationships with other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is value vector computed? What is it's dimension?\n",
    "\n",
    "1. From a corpus, its embedding matrix is calculated, say X.\n",
    " $$X_{N\\times d} \\ \\begin{cases}N - \\text{number of unique words in the corpus}\\\\ d - \\text{dimension of the word embeddings}\\end{cases}$$\n",
    "2. Then a matrix of parameters $W_V$ comes into picture. It will act as a linear transformation that transforms the word embedding into a value vector $d_k$\n",
    "$${W_V}_{d\\times d_v}\\begin{cases}d - \\text{dimension of the word embeddings}\\\\ d_v - \\text{dimension of the value vectors}\\end{cases}$$\n",
    "Using $W_V$, the value vector corresponding of any token (an embedding vector) can be calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Using a flow chart, illustrate the calculation of attention scores.\n",
    "\n",
    "1. Query vector for the current token is calculated\n",
    "2. Key vectors of all previous tokens in the sentence are calculated\n",
    "3. Value vectors of all previous tokens in the sentence are calculated\n",
    "4. for each key vector :\n",
    "    - take dot product of it with the current query vector to get its attention score\n",
    "    - scale the score by dividing with $d_k$ viz. the dimension of the query & key vector\n",
    "5. all the calculated scores are fed into a softmax function. It is a multivariate vector-valued function.\n",
    "$$\\text{all attention scores } \\to \\boxed{\\text{ softmax function }} \\to \\text{ normalized scores = attention weights}$$\n",
    "$$(\\text{score}_i)_{i=1}^t \\to \\boxed{\\text{ softmax function }} \\to \\left(\\frac{e^{score_i}}{\\sum_{j=1}^te^{score_j}}\\right)_{i=1}^t$$\n",
    "t denotes the total number of previous tokens at time $t$.\n",
    "\n",
    "6. Value vectors of all the previous tokens are linearly combined using their current attention scores to get the output. This linear combination represents the context vector for the current token, capturing relevant information from the previous tokens.\n",
    "\n",
    "$$\\text{Output at time t} = \\sum_{i=1}^t\\text{weight}_iV_i$$\n",
    "\n",
    "7. The resulting output vector represents the current token, enriched with information from other tokens in the sequence. If needed, this **output can be considered as the updated version of the current token** (as earlier it was represented using a context-free embedding) & can be passed through a feedforward neural network or an activation function (like ReLU) for further processing\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** We have seen the calculation of attention weights. Why is it time-dependent?\n",
    "\n",
    "* at each time step, a new token is being considered for processing by the layer, hence a new query vector is introduced at each time step\n",
    "* any query vector will interact differently with key vectors, hence attention scores will get changed at each time step\n",
    "* remember the key vector signifies the importance of an input element or token in relation to a specific query in the sequence processing. The specific query is different at any instance of time\n",
    "* each time a new word appear, the contextual importance of other input words changes, this change is captured using the attention scores\n",
    "* hence at every time step, context keep changing so does the attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** If attention weights keep changing at every time step, then what is the layer trying to learn during training?\n",
    "\n",
    "- The attention mechanism computes the time-varying coefficients (attention weights) based on **query, key, and value** representations of the input data.\n",
    "- These representations are obtained by **linear transformations** using weight matrices, $W_Q$, $W_K$ and $W_V$, which are learnable parameters.\n",
    "- During training, the model learns the values of these matrices that will allow it to compute queries, keys, and values effectively for the task at hand\n",
    "- The model learns to **adjust the query, key, and value transformations** so that the resulting attention weights highlight the most relevant parts of the input sequence for each output step.\n",
    "- In many cases, the output of an attention layer is passed on to further layers (such as in a multi-layer transformer). \n",
    "- If the attention mechanism is part of a larger model, such as a transformer encoder-decoder, the parameters in these additional layers are also learned during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain the significance of the linear transformations $W_Q$, $W_K$ & $W_V$.\n",
    "\n",
    "* While the attention coefficients vary depending on the input and context at each time step, the weight matrices $W_Q$, $W_K$, and  $W_V$ that determine how to construct the queries, keys, and values are learned and fixed after training. \n",
    "\n",
    "* These learned parameters **define the mechanism for computing attention weights**, enabling the model to **generalize to unseen sequences by dynamically adjusting focus** based on the learned transformations. \n",
    "\n",
    "* these transformations define how each input influences others in context giving flexibility in handling sequences of varying length and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** After training, the transformations $W_Q$, $W_K$ & $W_V$, remain constant model parameters. For any unseen sentence, the word embeddings of words in it also do not change. Now that we have discussed the calculation of Q, K & V, do you think there is a redundancy while calculating K & V for finding out attention scores? What is happening in reality?\n",
    "\n",
    "Let, \"cat sat on the soft mat\" be the sentence to be processed by a pre-trained attention layer.\n",
    "\n",
    "1. For each token in the sentence the model looks up the corresponding word embeddings using the fixed, shared embedding matrix. This embedding lookup happens **only once per token per sentence**.\n",
    "\n",
    "2. Since, the $W_K$ of the pretrained model is fixed, and each word embedding in this sentence is fixed the key vector for each token will remain constant throughout the processing of this sentence.\n",
    "\n",
    "3. \n",
    "    - When processing \"cat\" (first word), the model generates its query, key, and value vectors.\n",
    "    - When moving to \"sat\" (second word), it generates a new query vector for \"sat\" but **reuses the already-computed key and value vectors** for \"cat\" since those vectors don’t change within this sentence.\n",
    "    - For \"on\" (third word), the process is the same, the model generates a query for \"on\" and reuses the key and value vectors for both \"cat\" and \"sat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE :** Even though the key and value vectors for each word stay fixed for that sentence, the **attention scores and context vectors can still vary with each new word**. This happens because the query vector for each word changes, creating different interactions with the fixed key vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Extend the student-teacher-lesson analogy discussed previously into the attention layer concept.\n",
    "\n",
    "* The teacher provides all past lessons’ notes to the student\n",
    "* for each new lesson, the student selectively chooses which previous notes to review\n",
    "* The student doesn’t have to focus only on recent lessons; instead, they can scan through all previous notes, applying varying levels of attention to each based on relevance\n",
    "* The student’s “attention” isn’t limited to recent notes; they can dynamically weight notes from any point in time depending on their relevance to the current lesson \n",
    "* This flexibility allows the student to draw connections from any past lesson to make sense of the current one, which enables deeper and broader understanding without the rigid restrictions of a tapped delay line or the sequential dependency of time-delayed feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **SUMMARIZE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tapped Delay Line** : Limited memory window, simple retention of recent notes only\n",
    "\n",
    "**Time-Delayed Feedback** : Cyclic, cumulative understanding, sequentially built\n",
    "\n",
    "**Attention Mechanism** : Non-sequential, selective attention to all past notes based on relevance to the current lesson, enabling flexible and contextual understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain the simplest possible RNN with a diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/RNN_folded.jpeg\" alt=\"image description\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this simple RNN has only 1 hidden layer viz. represented as **a**\n",
    "* the hidden layer uses hyperbolic tan as the activation function\n",
    "* there is a time-delayed feedback path viz. shown in red\n",
    "* this path stores the current value from the hidden layer **h** at each time and delays it, so that it can be sent back to the input layer to concatenalte with a new input arriving at the next time instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What happens if this RNN is used to process the sequence of input vectors $\\{x_1,x_2, \\dots, x_T\\}$, provided, $h_0$ represents the initial status from the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let $t$ denote the time steps while which the RNN is running. Then $t\\in \\{1,2,\\dots, T\\}$\n",
    "* At any instance of time , the input vector reaches the hidden layer after a linear transformation as follows :\n",
    "$$a_t = W_1[x_t; h_{t-1}] + b_1$$\n",
    "\n",
    "* this linearly transformed input undergoes a non-linear transformation in the hidden layer as follows :\n",
    "$$h_t = tanh(a_t)$$\n",
    "\n",
    "* $h_t$ will be sent to the output layer to generate the output as :\n",
    "$$y_t = W_2h_t + b_2$$\n",
    "\n",
    "* $h_t$ will also be stored and delayed in the feedback path and will be concatenated with the next input $x_{t+1}$ during the next time step viz. $t+1$\n",
    "\n",
    "* $W_1$, $W_2$, $b_1$ & $b_2$ denotes the parameters used in the 2 full connections of the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain this diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/RNN_unfolded.jpeg\" alt=\"image description\" width=1000 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* convinient way to analyze the behavior of the recurrent feedback in an RNN is to unflod the recursive computation along the time steps for the whole input sequence.\n",
    "* RNN = duplicated non-recurrent networks for every time instance, each passing a message to its successor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the major problem with a RNN.\n",
    "\n",
    "* if RNN is used to process a long input sequence, this non-recurrent network becomes very deep\n",
    "* if T is very large, the length of the above unfolded diagram will be really large\n",
    "* theoretically RNN are powerful models suitable for all types of sequences\n",
    "* in practice the learning of RNNs is very difficult due to the deep structure as a result of the feedback paths\n",
    "* Empirical results have shown that, simple RNN structure as discussed above are only **good at modeling short-term dependency in sequences** \n",
    "* they fail to capture any dependency that spans a long distance in input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the structure variations of RNNs that resolves the above issue.\n",
    "\n",
    "1. Long short-term memory (LSTM)\n",
    "2. Gated Recurrent Units (GRU)\n",
    "3. Higher Order Recurrent Neural Networks (HORNN)\n",
    "4. Bidirectional RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Intro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the **basic ideas** behind LSTMs?\n",
    "\n",
    "* address the shortcomings of traditional RNNs \n",
    "* incorporate memory cells and gating mechanisms in the RNN\n",
    "* allow the network to learn and retain information over long sequence\n",
    "* use a combination of forget, input, and output gates to enable the model to control the flow of information, ensuring that relevant context is preserved while unimportant information is discarded​\n",
    "* address the VG problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the **main crux idea** of a LSTM network?\n",
    "\n",
    "* RNNs have only one feedback path and it captures only short-term memory\n",
    "* LSTMs use 2 different paths to retain short-term memory and long-term memory seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** LSTM is a modification of traditional RNN. List out the **modifications**.\n",
    "\n",
    "* memory cells\n",
    "* gates\n",
    "* cell state in addition to the hidden state\n",
    "* usage of sigmoid activation function for cell state and output activation in addition to tanh for hidden state and gates activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What do you mean by a **neuron** in LSTM network?\n",
    "\n",
    "* In a standard neural network, a \"neuron\" refers to a single computational unit\n",
    "* in LSTMs, a \"neuron\"/\"unit\" typically refers to an LSTM cell, each with its own gates (input, forget, and output gates)\n",
    "* Each LSTM cell is a complex structure that : \n",
    "    - performs multiple computations \n",
    "    - manages memory over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What do you mean by a **layer** in an LSTM network?\n",
    "\n",
    "* In a single LSTM layer, there are as many \"neurons\"/\"units\" as specified by the number of units or number of hidden dimensions when defining the LSTM layer \n",
    "* For example, if we specify an LSTM layer with 128 units, that layer will have 128 LSTM cells at each time step\n",
    "* Each LSTM layer consists of multiple LSTM cells, all operating in parallel for each time step in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What do you mean by an LSTM **neural network**?\n",
    "\n",
    "* an LSTM network refers to a neural network with one or more LSTM layers\n",
    "* You can have a single LSTM layer, or you can stack multiple LSTM layers on top of each other to create a deeper LSTM network\n",
    "* This layered structure enhances the network's capacity to capture complex, hierarchical patterns in sequential data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How does an **LSTM layer process a sequence**?\n",
    "\n",
    "For a given input sequence(a sentence viz. a sequence of words) with T time steps( or elements) :\n",
    "  - at each time step t:\n",
    "       - The layer consists of multiple LSTM cells (let's say n cells) that process the same input vector(a word embedding ) $x_t$ simultaneously.\n",
    "       - Each of these cells produces its own hidden state output based on the input $x_t$ and the previous hidden and cell states.\n",
    "* The output of the LSTM layer at time step t is thus a collection of the hidden state output vectors from all n cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Draw a labelled diagram that represents an LSTM cell's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/LSTM.jpeg\" alt=\"image description\" width=700 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/lstm1.jpg\" alt=\"image description\" width=250 height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/lstm.jpg\" alt=\"image description\" width=500 height=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** At any instance of time, what are the data being fed into an LSTM neuron?\n",
    "\n",
    "1. $X_t$ - current element from the input sequence (a vector)\n",
    "2. $h_{t-1}$ - the hidden state output from previous time step which was delayed and stored for use in this step (a vector), that actually represents the short term memory \n",
    "3. $C_{t-1}$ - the cell state output from previous time step which was delayed and stored for use in this step (a vector), that actually represents the long term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Forget Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the components in the forget gate? What is it's purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Components :**\n",
    "\n",
    "1. 2 weight matrices $W_{f1}$, $W_{f2}$\n",
    "2. a bias vector $B_f$\n",
    "3. a sigmoid activation function \n",
    "4. current input $X_t$, is given as input\n",
    "5. previous hidden state output $h_{t-1}$ is also given as input\n",
    "6. outputs a vector with each component between 0 and 1 that denotes the fraction of current Long-term memory( viz. a vector of same dim ) to be remembered further\n",
    "\n",
    "**Diagram :**\n",
    "\n",
    "$$\\begin{cases}1. & \\text{Current Input; } X_t \\\\ 2. & \\text{Previous hidden state output; } h_{t-1} \\end{cases} \\to \\boxed{\\text{ Forget Gate }} \\to \\text{Fraction of current LTM to be remembered}$$\n",
    "**Math :**\n",
    "\n",
    "$$\\text{Forget Gate Activation } = \\sigma(W_{f1}h_{t-1} + W_{f2}X_t + B_f) = f_t$$\n",
    "\n",
    "This output is then multiplied by previous LTM vector to factor it out as :\n",
    "\n",
    "$$\\text{Prev. LTM to be remembered further} = C_{t-1}\\cdot f_t$$\n",
    "\n",
    "**Purpose :**\n",
    "1. Selective memory retention\n",
    "2. Prevention of memory overload\n",
    "3. Handle long-term dependencies\n",
    "4. Enable dynamic memory\n",
    "5. Prevent gradient problems\n",
    "6. Enhance model flexibility\n",
    "7. Efficient resource usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How does LSTM serve each of its purpose?\n",
    "\n",
    "1. **Selective Memory Retention**: \n",
    "    - By adjusting values in the forget gate output (from 0 to 1), the network can decide which parts of past information are important for future predictions and which can be discarded.\n",
    "\n",
    "2. **Prevention of Memory Overload**: \n",
    "    - retaining too much past information can lead to memory overload\n",
    "    - irrelevant or outdated information clutters the cell state \n",
    "    - allows the LSTM to forget less useful information\n",
    "    - helps the model focus on more relevant data\n",
    "\n",
    "3. **Handling Long-Term Dependencies**: \n",
    "   - enables the LSTM to learn long-term dependencies in sequential data\n",
    "   - controls which information persists over time, the network can remember significant information across long gaps\n",
    "   - useful in tasks like language modeling or time-series forecasting\n",
    "\n",
    "4. **Enabling Dynamic Memory**: \n",
    "    - Different types of data sequences have varying needs for memory retention\n",
    "    - it adapts dynamically based on the input at each time step, allowing the LSTM to adjust how much of the past information it keeps as context changes\n",
    "\n",
    "5. **Preventing Gradient Explosions and Vanishing**: \n",
    "    - helps mitigate the vanishing gradient problem in RNNs \n",
    "    - it ensures that unnecessary information is \"forgotten\" early on\n",
    "    - hence, prevents gradients from becoming too small or big\n",
    "    - helps gradients propagate more effectively through longer sequences, aiding learning stability.\n",
    "\n",
    "6. **Enhanced Model Flexibility**: \n",
    "    - adds flexibility by allowing the LSTM to learn which parts of the memory are useful at any point in the sequence\n",
    "    - helps the LSTM perform well on complex, real-world tasks where relevant information can vary significantly over time\n",
    "\n",
    "7. **Efficient Resource Use**: \n",
    "    - reduces the computational burden on the cell state\n",
    "    - particularly beneficial for long sequences, where retaining all past information would be computationally wasteful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What essentially is the forget gate trying to control?\n",
    "\n",
    "* This gate determines what information should be discarded from the cell state or the available LTM. \n",
    "* It also uses a sigmoid function to produce values between 0 and 1, which indicate how much of each element in the cell state should be kept or removed\n",
    "* A value of 0 means \"completely forget,\" while a value of 1 means \"keep it all.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the components in the input gate? What is it's purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Components :**\n",
    "\n",
    "  * UNIT - 01\n",
    "    - 2 weight matrices $W_{i11}, \\ W_{i12}$\n",
    "    - a bias vector $B_{i1}$\n",
    "    - a sigmoid activation function\n",
    "    - $X_t$, the current input vector is inputted\n",
    "    - $h_{t-1}$, the previous hidden state output is also an input\n",
    "    - outputs a vector $i_t$, whose each component is a number b/w 0 & 1 that indicates the fraction of potential LTM from current input to be remembered further\n",
    "\n",
    "  * UNIT - 02\n",
    "    - 2 weight matrices $W_{i21}, \\ W_{i22}$\n",
    "    - a bias vector $B_{i2}$\n",
    "    - a tanh activation function\n",
    "    - $X_t$, the current input vector is inputted\n",
    "    - $h_{t-1}$, the previous hidden state output is also an input\n",
    "    - outputs a vector $\\tilde{C}_t$ whose each component is a number b/w -1 & 1 that indicates the potential LTM from current input\n",
    "\n",
    "**Math :**\n",
    "\n",
    "$$\\text{Input Gate Activation } = \\sigma(W_{i11}h_{t-1} + W_{i12}X_t + B_{i1}) = i_t$$\n",
    "\n",
    "$$\\text{Candidate Cell State } = tanh(W_{i21}h_{t-1} + W_{i22}X_t + B_{i2}) = \\tilde{C}_t$$\n",
    "\n",
    "**Purposes :**\n",
    "\n",
    "1. controls the flow of new information into the cell state\n",
    "2. selectively filters input\n",
    "3. supports long-term dependencies\n",
    "4. prevents memory overwriting\n",
    "5. adapts memory updates dynamically\n",
    "6. enhances temporal pattern recognition\n",
    "7. balances short-term and long-term information\n",
    "8. modulates the learning signal during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What does input activation and candidate cell state imply?\n",
    "\n",
    "* **input gate activation** is a vector that determines which new information from the candidate cell state $\\tilde{C}_t$ will be added to the cell state\n",
    "* **candidate cell state** represents the \"potential\" memory that could be added to the cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What does potential long term memory signify?\n",
    "\n",
    "* represents the candidate cell state, the \"new information\" that could be added to the cell's memory\n",
    "* signifies a potential update to the long-term memory (cell state) based on the current input and previous hidden state, \n",
    "* but this update is only incorporated into the actual cell state if the input gate allows it\n",
    "* generated by applying a **tanh** activation function to a weighted combination of the current input $X_t$ and the previous hidden state $h_{t-1}$. The tanh function scales the candidate values between -1 and 1, making them suitable for updating the cell state.\n",
    "* represents new information from the current input that the LSTM might want to \"remember\" for future steps\n",
    "* If the input gate’s output is close to 1 for a specific piece of information, it suggests that this new information is important and should be added to the cell state\n",
    "* If the input gate output is closer to 0, that information will be suppressed or only partially added.\n",
    "* helps the LSTM incorporate relevant details from each time step, allowing it to gradually build up or update its memory with useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What essentially is the input gate trying to control?\n",
    "\n",
    "* This gate controls how much new information from the current input should be added to the cell state or the LTM. \n",
    "* It uses a sigmoid activation function to determine which values to update and a tanh activation function to create a vector of new candidate values that could be added. \n",
    "* The input gate helps the LSTM decide what information from the input is relevant and should be incorporated into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Output Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the components in the output gate? What is it's purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Components :**\n",
    "\n",
    "* UNIT - 01:\n",
    "   - 2 weight matrices $W_{o1}$, $W_{o2}$\n",
    "   - a bias vector $B_o$\n",
    "   - a sigmoid activator\n",
    "   - takes $X_t$, $h_{t-1}$ as inputs\n",
    "   - outputs $o_t$, the output gate activation, a vector\n",
    "* UNIT - 02:\n",
    "   - no weight matrices or bias vector\n",
    "   - a hyperbolic tan activator\n",
    "   - takes $o_t$, $C_t$, the new LTM vector as inputs\n",
    "   - outputs $h_t$, the current hidden state activation, a vector\n",
    "\n",
    "**MATH :**\n",
    "\n",
    "$$o_t = \\sigma(W_{o1}h_{t-1} + W_{o2}X_t + B_o)$$\n",
    "$$h_t = o_t\\cdot tanh(C_t)$$\n",
    "\n",
    "**PURPOSES :**\n",
    "\n",
    "* controls the final output from the cell\n",
    "* selectively filters the cell state information\n",
    "* supports sequential predictions\n",
    "* maintains long-term dependencies\n",
    "* prevents information overload\n",
    "* modulates gradient flow\n",
    "* enhances representational power\n",
    "* enables smooth transitions between states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What does $o_t$ and $h_t$ signify?\n",
    "\n",
    "* $o_t$ - represents the fraction of short term memory to be remembered further\n",
    "* $h_t$ - represents updated version of the short term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What essentially is the output gate trying to control?\n",
    "\n",
    "* This gate controls what part of the cell state/LTM should be output to the next layer or the next time step. \n",
    "* It again uses a sigmoid function to decide which information to pass through and a tanh function to scale the cell state values to be between -1 and 1. \n",
    "* The output gate essentially decides how much of the cell state should influence the output at that time step.\n",
    "* control the flow of information from the long-term memory (cell state) to the short-term memory (output), ensuring that only relevant information is passed along in the sequence processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** \"Each gate in an LSTM neuron can be thought of as a tiny neural network\". Justify\n",
    "\n",
    "* each gate has its own set of parameters (weights and biases) and uses an activation function to transform the inputs. \n",
    "* These gates play specific roles in controlling the flow of information in and out of the cell, much like a neural network transforms inputs to achieve a specific function\n",
    "* During backpropagation, the network adjusts these weights and biases to improve the LSTM’s performance in retaining, forgetting, or outputting the right information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is a memory cell in an LSTM?\n",
    "\n",
    "* Its not a traditional neuron\n",
    "* nor is it a traditional standalone layer in the LSTM network\n",
    "* rather, it is a functional unit in an LSTM layer\n",
    "* conceptually, it can be thought of as the layers memory\n",
    "* it has a tangible implementation in code and in neural architectures of LSTM\n",
    "* it represents a vector of values that holds the memory state over time\n",
    "* it is a part of the internal structure of an LSTM layer, coexisting with gate components of it\n",
    "* designed to store and manage information over long sequences\n",
    "* allows the network to retain knowledge across multiple time steps\n",
    "* The memory cell carries information that can be modified by carefully controlled mechanisms, enabling it to selectively remember or forget relevant details as needed\n",
    "* persistent, adaptable store of information across time steps, managed by gates that regulate what to remember, forget, and output at each step\n",
    "* makes LSTMs robust in handling long-range dependencies in sequential data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
