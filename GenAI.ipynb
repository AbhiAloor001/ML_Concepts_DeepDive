{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gen AI & LLMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** \"Find out whether the given email is legitimate or spam\". Give a High-level overview of this problem statement.\n",
    "\n",
    "* It is a binary classification problem\n",
    "* Input is text, should be subjected to numerical transformation\n",
    "* supervised learning problem since email label is present in training data\n",
    "\n",
    "**Features**\n",
    "\n",
    "1. Timestamp\n",
    "2. From\n",
    "3. Subject (text field)\n",
    "4. Mail body (text field)\n",
    "5. cc list\n",
    "6. Category (personal, domain based, social, promotional etc.)\n",
    "\n",
    "**Label**\n",
    "\n",
    "1. Spam/ Not Spam\n",
    "\n",
    "**Task (Step by Step)** \n",
    "\n",
    "1. Identify the input and output --> Use 3rd and 4th feature combined(feature selection) to determine label\n",
    "2. EDA --> Count plot, word cloud\n",
    "3. Train-test split\n",
    "4. NLP of training data (**fit_transform**)\n",
    "   * Clean the text data \n",
    "   * Transform the cleaned input part of training dataset( use any of the text to num algo )\n",
    "5. Repeat step 4 on text data also (**transform**)\n",
    "6. Model building step using the transformed train set\n",
    "7. Train score calculation (Mock test)\n",
    "8. Test score calculation (Evaluation step)\n",
    "9. Inference - putting model in real time\n",
    "10. deployment of the model\n",
    "\n",
    "**NOTE** : The model = (pre-processing component + classification component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** After train score calculation, what to do if you findout that the model has not learned anything?\n",
    "\n",
    "* change the algorithm\n",
    "* hyperparameter tuning(validation score vs test score)\n",
    "* alter pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** After evaluation & before deployment of a ML model, there is an inference step. Justify\n",
    "\n",
    "The inference step after model evaluation and before deployment is essential for the following reasons:\n",
    "\n",
    "1. **Model Performance Validation**: \n",
    "* ensures that the trained model generalizes well to unseen data (test data)\n",
    "* you verify that the model predictions meet performance expectations (accuracy, precision, recall, etc.), confirming the model is ready for real-world data.\n",
    "\n",
    "2. **Operational Readiness**: \n",
    "* Inference tests the model’s ability to function in a live environment\n",
    "* helps simulate the actual deployment context, including handling data inputs and system integration, ensuring the model will work smoothly after deployment.\n",
    "\n",
    "3. **Debugging and Optimization**: \n",
    "* allows developers to catch potential issues such as latency, memory usage, or incorrect predictions in real-world conditions\n",
    "* provides an opportunity to fine-tune and optimize the model before deployment\n",
    "\n",
    "4. **Business Value Validation**: \n",
    "* performing inference on a separate validation or test set, stakeholders can evaluate if the model provides the desired business outcomes, ensuring alignment with business goals before full-scale deployment.\n",
    "\n",
    "Thus, inference acts as a **bridge between evaluating model performance and the practical, efficient use of the model in production**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the major steps in text preprocessing.\n",
    "\n",
    "1. Lower casing\n",
    "2. Tokenization\n",
    "3. Removing Punctuation and special characters\n",
    "4. Removal of stop words\n",
    "5. Stemming/ Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **TOKENIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is tokenization?\n",
    "\n",
    "* process of splitting text into smaller units called tokens \n",
    "* These tokens can be words, phrases, or even characters, depending on the level of tokenization\n",
    "* crucial step in text preprocessing for NLP tasks\n",
    "* By breaking down text into tokens, ML models can work with manageable and meaningful parts of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the different types of tokenization?\n",
    "\n",
    "* **Word Tokenization** : \n",
    "    * Breaks text into individual words \n",
    "    * \"I love NLP\" becomes [\"I\", \"love\", \"NLP\"].\n",
    "* **Sentence Tokenization** : \n",
    "    * Splits text into sentences \n",
    "    * \"I love NLP. It's fun!\" becomes [\"I love NLP.\", \"It's fun!\"].\n",
    "* **Character Tokenization** : \n",
    "    * Splits text into individual characters\n",
    "    * \"love\" becomes [\"l\", \"o\", \"v\", \"e\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Which module in **nltk** library contains functions for tokenization?\n",
    "\n",
    "* tokenize module of nltk library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is word tokenization enabled in Python using nltk? Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data files (only needed for the first time)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"I love NLP! It's fascinating.\"\n",
    "\n",
    "# Perform word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# output\n",
    "['I', 'love', 'NLP', '!', 'It', \"'s\", 'fascinating', '.']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is sentence tokenization implemented in Python using NLTK? Give example.\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"I love NLP. It's a fascinating field of study.\"\n",
    "\n",
    "# Perform sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "\n",
    "# output\n",
    "['I love NLP.', \"It's a fascinating field of study.\"]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **PUNCTUATIONS/SPECIAL CHARACTERS REMOVAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why Remove Special Characters and Punctuation?\n",
    "\n",
    "* **Noise Reduction** : Special characters and punctuation may not contribute meaningfully to the task, and removing them reduces noise in the data.\n",
    "* **Simplifies Text** : Focusing on words without clutter simplifies the input for machine learning models.\n",
    "* **Uniformity** : It brings uniformity to the dataset, especially when handling large corpora of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why Remove Special Characters After Tokenization?\n",
    "\n",
    "* **Flexibility** : By tokenizing first, you have more granular control. For example, some punctuation (e.g., emoticons, hashtags) might carry meaning, and you can choose which tokens to keep or remove.\n",
    "* **Handling Contractions** : In English, contractions like \"don't\" and \"it's\" include punctuation. Removing punctuation before tokenization can complicate things (e.g., turning \"don't\" into \"don\" and \"t\").\n",
    "* **Avoid Over-removal** : If you remove special characters before tokenization, you risk stripping away meaningful punctuation marks or symbols that should be preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the methods that can be used for removal of punctuations and special characters from tokenized data.\n",
    "\n",
    "1. Regex method\n",
    "2. Python string library method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to use Regex for special character removal?\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "tokens = ['Hello', '!', '!', '!', 'How', \"'s\", 'everything', 'going', '?', '#', 'Exciting', ':', ')']\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "cleaned_tokens = [token for token in tokens if re.match(r'^\\w+$', token)]\n",
    "\n",
    "print(cleaned_tokens)\n",
    "\n",
    "# output\n",
    "['Hello', 'How', 'everything', 'going', 'Exciting']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to use Python string library for special character removal?\n",
    "\n",
    "```python\n",
    "import string\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation using string.punctuation\n",
    "cleaned_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "print(cleaned_tokens)\n",
    "\n",
    "# output\n",
    "['Hello', 'How', \"'s\", 'everything', 'going', '#', 'Exciting']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **DEALING WITH STOP WORDS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are stop words?\n",
    "\n",
    "* common words in a language that are often removed from text data during NLP tasks\n",
    "* they usually don't carry significant meaning or contribute to the analysis\n",
    "* These words are frequently used in the text \n",
    "* they don’t provide much context or information in tasks like \n",
    "    * text classification\n",
    "    * information retrieval\n",
    "    * sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give examples of stop words.\n",
    "\n",
    "In English, stop words typically include:\n",
    "\n",
    "1. **Articles** : the, a, an\n",
    "2. **Pronouns** : he, she, it, they\n",
    "3. **Conjunctions** : and, or, but\n",
    "4. **Prepositions** : in, on, at, over, under, after, before\n",
    "5. **Auxiliary verbs** : is, am, are, was, were, be, has, have, had"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why Remove Stop Words?\n",
    "\n",
    "* **Reduce Noise** : Stop words don’t add significant meaning, so removing them helps reduce noise in the text\n",
    "* **Improve Efficiency** : reduces the number of tokens (words) that a machine learning model needs to process\n",
    "* **Focus on Important Words** : for sentiment analysis, keywords like \"great,\" \"bad,\" etc., are more useful than \"is\" or \"the\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Which module in NLTK contains stop words data?\n",
    "\n",
    "* the **corpus** module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give an example on how to implement stop words removal in Python using NLTK.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords dataset (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the text\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "\n",
    "# output\n",
    "Original Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
    "Filtered Tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **STEMMING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Define stemming.\n",
    "\n",
    "* process of reducing words to their base or root form by removing suffixes or prefixes, \n",
    "* it enables the grouping of related words for analysis\n",
    "* eg. the words \"running,\" \"runner,\" and \"ran\" can all be reduced to the root word \"run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the major algorithms to implement stemming.\n",
    "\n",
    "1. Porter stemmer\n",
    "2. Lovins stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Idea behind Porter stemmer algo.\n",
    "\n",
    "The algorithm consists of multiple steps, each containing a set of rules to handle specific suffixes. These steps are applied in a sequential manner, where the output of one step may serve as the input for the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 1: Removing Plurals and Past Tenses**\n",
    "\n",
    "Rules are applied to remove common suffixes like -s, -es, -ed, and -ing\n",
    "\n",
    "**Step 2: Handling Verb Suffixes**\n",
    "\n",
    "Additional suffixes related to verb forms are handled, like -ly, -ness, and -ment\n",
    "\n",
    "**Step 3: Removing Derivational Suffixes**\n",
    "\n",
    "Further rules target suffixes such as -er, -est, and -ful to reduce derived forms to their base form\n",
    "\n",
    "**Step 4: Handling Special Cases**\n",
    "\n",
    "The algorithm includes specific rules to handle exceptions or irregular forms that don't follow standard suffix patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Which NLTK module has function for implementing Porter stemmer algo?\n",
    "\n",
    "* the stem module in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to implement Porter stemmer algo in Python using NLTK?\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "tokens = [stemmer.stem(token) for token in tokens]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **LEMMETIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Define lemmatization.\n",
    "\n",
    "* process of reducing words to their base or dictionary form (lemma) by considering the context and meaning of the word\n",
    "* Unlike stemming, which often removes suffixes arbitrarily, lemmatization uses linguistic knowledge to ensure that the root form is a valid word, such as converting \"better\" to \"good\" or \"running\" to \"run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to implement lemmatization in python using NLTK?\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**List Comprehension is widely used in Text Preprocessing Tasks!!!**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is NLP?\n",
    "\n",
    "* Natural Language Processing\n",
    "* the input text data in the above problem is \"Natural Language\" used by humans\n",
    "* The conversion of N.L into model-friendly format(pandas friendly tabular format) is NLP\n",
    "\n",
    "It is a study of :\n",
    "* processing\n",
    "* analyzing\n",
    "* understanding(by means of a built model) &\n",
    "* generating(by means of a built model)\n",
    "\n",
    "text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing\n",
    "___\n",
    "1. Cleaning\n",
    "  * special char removal\n",
    "  * lower\n",
    "  * stop word removal\n",
    "  * lemmatization\n",
    "\n",
    "2. Vectorization\n",
    "  * text to num conversion (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out steps involved in Text data cleaning \n",
    "\n",
    "* convert into lowercase\n",
    "* remove stop words\n",
    "* stemming, lemmetization\n",
    "* tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Tabulate the algorithms and their corresponding Libraries.\n",
    "\n",
    "|Algo|Library|\n",
    "|-----|----|\n",
    "|BOW, TFIDF|sklearn|\n",
    "|Word2Vec, GLoVe, FastText|gensim|\n",
    "|RNN, CSTM, GRU|Tensorflow, Keras, Pytorch|\n",
    "|BERT, GPT, LLMs|Hugging Face, Openai, Google genai, anthropic|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the AI tasks that are dealt with by NLP domain.\n",
    "\n",
    "1. **Text Classification** : \n",
    "   * Spam detection\n",
    "   *  Sentiment analysis\n",
    "   * Adult content filtering etc. \n",
    "   * Goal is to classify the entire piece of text into categories\n",
    "\n",
    "2. **Information extraction** :\n",
    "   * Goal is to ectract structured info from unstructured text data \n",
    "   * Named entity recognition\n",
    "     - It is a sequence classification task\n",
    "     - goal is to classify each element in a sequence into categories\n",
    "   * Part of speech tagging\n",
    "\n",
    "3. **Information Retrieval**\n",
    "   * given a user query, the model should be able to retrieve a relvant piece of info from a huge repo like documents / websites\n",
    "   * retrieve dics/ info that best matches a user's query\n",
    "\n",
    "4. **Text summarization**\n",
    "   * News feed creation\n",
    "   * case report summarization in a big law firm\n",
    "   * such models are Gen AI model\n",
    "   * text doc --> model --> summary of the doc\n",
    "\n",
    "5. **Machine Translation**\n",
    "\n",
    "6. **Q and A Systems**\n",
    "   * automated systems to answer\n",
    "     * customer query\n",
    "     * weather updates\n",
    "     * healthcare chatbots \n",
    "   * question reaches model --> model does information **retrieval** --> model performs info **extraction** --> model **generates**(Gen AI) the quality answer based on the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Show an example that illustrates the purpose of NER Information extraction.\n",
    "\n",
    "* sentence : \"Apple is lookimg to launch M4 Macbooks for 1500$\"\n",
    "\n",
    "Entities :\n",
    "\n",
    "1. Apple = Oraganization\n",
    "2. M4 Macbooks = Product\n",
    "3. 1500$ = Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Show an example that illustrates the speech tagging information extraction.\n",
    "\n",
    "* senetence = \"She runs fast.\"\n",
    "\n",
    "Parts of speech :\n",
    "\n",
    "1. She = pronoun\n",
    "2. runs = verb\n",
    "3. fast = adverb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why is NLP hard?\n",
    "\n",
    "1. **Ambiguity** : refers to the uncertainity in meaning\n",
    "   * \"The chicken is ready to eat\"\n",
    "   * \"The car hit the pole while it was moving\"\n",
    "2. **Complexity of Representation** : refers to poem, sarcasm, phrases etc.\n",
    "   * \"You have a football game tomorrow. Break a leg!\"\n",
    "   * \"Yeah, right, because that worked so well last time\"( sarcasm )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text to Num Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectoization techniques / Language Representation techniques / text representation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the possible challenges or drawbacks while vectorizing text.\n",
    "\n",
    "1. Higher dimensionality of the vector\n",
    "2. Sparsity of the vector\n",
    "3. Vectors not able to capture the semantic relationships between the words\n",
    "4. Vectors that do not account for sequence of the words (the order in which they appear) in a sentence\n",
    "5. Vectors not being able to capture the context in which a word is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the problem with sparse vectors or matrix ?\n",
    "\n",
    "* visualization\n",
    "* memory consumption\n",
    "* computational complexity\n",
    "* curse of dimensionality\n",
    "* lack of interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Suggest a solution for this problem.\n",
    "\n",
    "* remove stop words\n",
    "* convert words to their root form\n",
    "* remove punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** BOW, TF-IDF vectorization techniques results in loss of sequence information in a text. What is the solution?\n",
    "\n",
    "* N-grams approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Assuming that all the vocabulary words can be represented using 2 dimensional vector representation, Which plot do you think make more sense?\n",
    "\n",
    "* scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give example where the vectorization technique's inability to capture sequence information in a sentence can be a problem. What is the problem?\n",
    "\n",
    "* Sentence 1 : This is my notebook.\n",
    "* Sentence 2 : Is this my notebook?\n",
    "\n",
    "Problem :\n",
    "\n",
    "1. loss of sequence information would mean that any model using such vector representations could \n",
    "   * misinterpret these two sentences as having the same meaning\n",
    "   * leading to incorrect predictions or responses in applications like \n",
    "       * sentiment analysis\n",
    "       * question answering\n",
    "       * chatbots\n",
    "2. Although these sentences contain exactly the same words, the sequence of the words completely changes the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why do you think that BOW and TF-IDF methods do not capture the sequence info in sentences?\n",
    "\n",
    "* they treat sentences as collections of words, disregarding their order\n",
    "* As a result, both sentences would have identical vector representations since these techniques only count occurrences of each word, not the position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why this happens with word 2 vec also?\n",
    "\n",
    "* represents individual words \n",
    "* it does not consider the sentence structure\n",
    "* it captures word relationships but ignores how words are sequenced to convey meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is Bag of Words?\n",
    "\n",
    "* it is an algorithm\n",
    "* it is used for making numerical representation of a text data\n",
    "* it transforms textual information into numerical data that ML algos can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the basic idea used BoW algo?\n",
    "\n",
    "* text (such as a sentence or document) = a collection of words \n",
    "* grammar, order, or context of the words are ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the major steps involved in BoW algo?\n",
    "\n",
    "**Step 1** - Text preprocessing\n",
    "\n",
    "**Step 2** - Create a vocabulary viz. a dictionary of words(tokens) in the corpus\n",
    "\n",
    "**Step 3** - For each document, count the occurrence of each word in the vocabulary. The result is a vector where each dimension represents the frequency of a word from the vocabulary in the document\n",
    "\n",
    "**Step 4** - Create a matrix(**Document Term matrix**) where each row represents a document, and each column represents a word from the vocabulary. The matrix contains word counts for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Which function in scikit-learn is used for implementing BoW?\n",
    "\n",
    "* CountVectorizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Which module in scikit learn has CountVectorizer function?\n",
    "\n",
    "* feature_extraction.text module of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Major tasks done by CountVectorizer function?\n",
    "\n",
    "* tokenizes the input text \n",
    "* It creates the document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the input parameters of CountVectorizer function.\n",
    "\n",
    "1. stop_words: Optionally remove common stop words.\n",
    "2. ngram_range: Specify the range of n-grams to consider (e.g., single words, pairs of words).\n",
    "3. max_features: Limit the number of features (words) to include based on frequency.\n",
    "4. lowercase: Convert all characters to lowercase to ensure uniformity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to implement BOW in Python using Scikit learn?\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is great for data science.\",\n",
    "    \"I enjoy learning new programming languages.\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to an array\n",
    "count_array = X.toarray()\n",
    "\n",
    "# Get the feature names (unique words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display results\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"Count Matrix:\\n\", count_array)\n",
    "\n",
    "# output\n",
    "Feature Names: ['data' 'enjoy' 'for' 'great' 'I' 'in' 'languages' 'love' 'new'\n",
    "                'programming' 'python' 'science']\n",
    "Count Matrix:\n",
    " [[0 0 0 0 1 1 0 1 0 1 1 0]\n",
    " [1 0 1 1 0 0 0 0 0 0 1 1]\n",
    " [0 1 0 0 1 0 1 0 1 1 0 0]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the features of BOW algo.\n",
    "\n",
    "* Simplicity: The model is straightforward and easy to interpret.\n",
    "* Easy to Implement: Libraries like sklearn provide direct methods to use BoW.\n",
    "* Works with Many ML Algorithms: BoW outputs vectors, which work well as inputs to most machine learning models.\n",
    "* Scalable: Suitable for large text datasets, especially with efficient sparse matrix representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Disadvantages of BOW algo?\n",
    "\n",
    "* ignores meaning. numerical representations are meaningless\n",
    "* semantic meaning not visible in the output\n",
    "* high dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is n-grams approach?\n",
    "\n",
    "* dimensionality of output vectors explode\n",
    "* \"I love programming in Python\"--> (\"I love\"), (\"love programming\"), (\"programming in\"), (\"in Python\") [bi-grams]\n",
    "* stop words removal, spcl char removal etc. are solutions to solve the sparsity, dimensionality problems. ie. text preprocessing is crucial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **TF IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Define term frequency (TF)\n",
    "\n",
    "* measures how frequently a term occurs in a document\n",
    "\n",
    "* It's typically calculated as:\n",
    "\n",
    "$$TF(t,d) = \\frac{\\# \\ of \\ term \\  t \\  in \\  d}{\\# \\ terms \\ in \\ doc \\ d}$$\n",
    "\n",
    "* it represents the presence or importance of a term in a document\n",
    "* it's value ranges between (0,1]\n",
    "* for any term t and document d, higher TF means that the term t is very significant in the document d\n",
    "\n",
    "​\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is inverse document frequency (IDF)?\n",
    "\n",
    "* This measures how important a term is across the entire corpus\n",
    "* If a term appears in many documents, it is less important, meaning it's less specific or, it carries lesser information to differentiate between the documents in the corpus\n",
    "\n",
    "$$IDF(t, D) = \\log\\left(\\frac{\\# \\ of \\ docs \\ in \\ the \\ corpus \\ D}{\\# \\ of \\ docs \\ that \\ has \\ t}\\right)$$\n",
    "\n",
    "* higher IDF score indicates that the term is rare across the entire corpus ie., the term is specific to certain documents in the corpus\n",
    "* Conversely, common terms (like \"the,\" \"is,\" etc.) will have low IDF scores because they appear in many documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/log.png\" alt=\"image description\" width=120 height=100>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to compute TF-IDF score?\n",
    "\n",
    "$$TF-IDF \\ score \\ = \\ TF(t,d)\\times IDF(t,D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** TF-IDF score depends on (or is a function of) ___ , ___ and ___ .\n",
    "\n",
    "* the term t, document d, corpus D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** In a specific corpus, TF-IDF score is a function of ___.\n",
    "\n",
    "* the term t and document d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What does a higher TF-IDF score imply?\n",
    "\n",
    "TF-IDF is high \n",
    "\n",
    "$\\Rightarrow$ TF is high & IDF is high \n",
    "\n",
    "$\\Rightarrow$ the term t appears more frequently in the document d, but the term t is rare in the whole corpus D\n",
    "\n",
    "$\\Rightarrow$ the term t is specific to the document d and also significant in the document \n",
    "\n",
    "$\\Rightarrow$ term t is likely to be an important keyword or topic in the document d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What does a lower TF-IDF score imply?\n",
    "\n",
    "TF-IDF is low \n",
    "\n",
    "$\\Rightarrow$ TF is low or IDF is low or both are low\n",
    "\n",
    "$\\Rightarrow$ the term t appears less frequently in document d or the term t is common in the whole corpus D or both\n",
    "\n",
    "$\\Rightarrow$ the term t is not specific to the document d or not significant in the document d or both\n",
    "\n",
    "$\\Rightarrow$ This indicates that the term is likely not a key topic or is generic in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** The terms $t_1$ and $t_2$ has same frequency in a document d. However IDF of $t_1$ is higher than $t_2$. What does that imply?\n",
    "\n",
    "1. **$t_1$ is rarer across the entire corpus**: A higher IDF value for $t_1$ suggests it appears in fewer documents in the corpus compared to $t_2$. Since IDF measures how unique a term is within the corpus, a higher IDF for $t_1$ implies it is more specific or unique across documents, while $t_2$ is more common.\n",
    "\n",
    "2. **$t_1$ is likely more important in document d**: Since both terms have the same frequency in d, the TF values for $t_1$ and $t_2$ are identical. However, the higher IDF for $t_1$ increases its overall **TF-IDF score** in d, making $t_1$ more significant than $t_2$ in identifying the unique content or focus of document d.\n",
    "\n",
    "3. **Interpretation in context**:\n",
    "   - $t_1$ likely represents a **more specific or specialized topic** in d since it is relatively rare across the corpus.\n",
    "   - $t_2$, being more common across documents, might be a broader or more general term that does not add as much unique value to d in comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why is Inverse of DF considered? Why not use DF?\n",
    "\n",
    "* to penalize common terms across the corpus as they do not give much information for distinguishing between documents\n",
    "* to highlight uncommon terms in the corpus allowing unique or specific terms in a document to stand out, making them more significant\n",
    "* Balancing local and global relevance of a term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Why Not Just Use TF? What is the need of IDF factor?\n",
    "\n",
    "* Without IDF, TF alone could make common words seem equally relevant across documents\n",
    "* TF-IDF approach refines the relevance by adjusting for words that are commonly used across the corpus, focusing on those that provide distinguishing information within each document\n",
    "* Using the inverse of document frequency allows us to emphasize terms that are not just frequently occurring but also unique or relevant to specific documents\n",
    "* makes TF-IDF a powerful technique for identifying key features that characterize documents in a meaningful way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give an example to demonstate the use of TF-IDF score.\n",
    "\n",
    "| Word              | Document 1 | Document 2 | Document 3 |\n",
    "|-------------------|------------|------------|------------|\n",
    "| data              | 0.1        | 0.2        | 0.15       |\n",
    "| machine learning  | 0.9        | 0.05       | 0.0        |\n",
    "| statistics        | 0.0        | 0.7        | 0.3        |\n",
    "\n",
    "* In Document 1, \"machine learning\" has a very high TF-IDF score, indicating that this term is central to its content.\n",
    "* In Document 2, \"statistics\" is crucial, while \"machine learning\" is not.\n",
    "* In Document 3, \"data\" is present but not significant enough to indicate a specific topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Which module in sklearn has the functionality to execute TF-IDF approach?\n",
    "\n",
    "* feature_extraction.text module of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give a brief overview about TfidfVectorizer.\n",
    "\n",
    "\n",
    "* It is a Scikit-Learn tool that transforms a collection of raw text documents into a matrix of TF-IDF features \n",
    "* it represents each document by the importance of each term\n",
    "* Purpose: Convert a text corpus into a numerical representation, capturing the relative importance of words across documents\n",
    "* Output: A sparse matrix where each row represents a document, and each column represents a term. Each element in the matrix is the TF-IDF score for that term in that document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the attrbutes of a TfidfVectorizer object.\n",
    "\n",
    "1. **documents** : A list of documents (as strings) to be transformed into TF-IDF scores\n",
    "\n",
    "2. **max_df** : Words that appear in more than this proportion of documents are ignored (helps to remove very common words)\n",
    "\n",
    "3. **min_df** : Words that appear in fewer than this proportion of documents are ignored (helps to remove rare words)\n",
    "\n",
    "4. **stop_words** : A list or language name (e.g., 'english') to filter out common stop words\n",
    "\n",
    "5. **ngram_range** : Specifies the range of n-grams (e.g., (1, 2) for unigrams and bigrams)\n",
    "\n",
    "6. **max_features** : Limits the number of features based on frequency; only the top N features are retained\n",
    "\n",
    "7. **vocabulary** : Manually provide a mapping of terms to indices (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the methods of TfidfVectorizer object.\n",
    "\n",
    "* **.fit(documents)** : Learns the vocabulary and IDF values from the documents\n",
    "\n",
    "* **.transform(documents)** : Transforms documents into the TF-IDF matrix using the learned vocabulary and IDF values\n",
    "\n",
    "* **.fit_transform(documents)** : Combines the fit and transform steps in a single call (most commonly used)\n",
    "\n",
    "* **.get_feature_names_out()** : Returns the list of terms in the vocabulary\n",
    "\n",
    "* **.idf_**: Contains the learned IDF vector with inverse document frequencies for each term in the vocabulary\n",
    "\n",
    "* **.vocabulary_** : A dictionary that maps terms to feature indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What happens when a list of strings is fed into the TfidfVectorizer.\n",
    "\n",
    "1. **Tokenization** - The text is split into individual tokens (words or phrases, based on ngram_range)\n",
    "\n",
    "2. **TF Calculation** - Term frequency for each term in each document is calculated\n",
    "\n",
    "3. **IDF Calculation** - The inverse document frequency of each term is computed across the corpus\n",
    "\n",
    "4. **TF-IDF Transformation** - Each term's TF is multiplied by its IDF to produce the TF-IDF score for that term in each document\n",
    "\n",
    "5. **Normalization** - Optionally, each row (document) vector is normalized to have unit Euclidean norm, which helps in scaling the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is TF-IDF text to num conversion done in Python using scikit-learn?\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, min_df=0.1)\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Access the feature names and idf values\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "\n",
    "# Convert to dense format for visualization (optional)\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Display the output\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dense_matrix, columns=feature_names)\n",
    "print(df)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the unique features of the TF-IDF approach.\n",
    "\n",
    "1. Dimensionality Reduction: Reduces the impact of common terms (stop words) by assigning them lower weights.\n",
    "2. Normalization: TF-IDF helps in normalizing the text data, making it easier to compare different documents.\n",
    "3. Useful for Information Retrieval: Effective for tasks like document classification, clustering, and search engines.\n",
    "4. Handling Sparse Data: It is effective in managing high-dimensional sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the demerits associated with the TF-IDF approach.\n",
    "\n",
    "* **Sparsity** : TF-IDF can result in sparse matrices, which can be computationally expensive to process.\n",
    "* **Ignores Word Order** : It treats words independently, losing context or meaning derived from word sequences.\n",
    "* **Not Semantic** : Does not account for synonyms or related words, which can lead to the loss of meaning.\n",
    "* **Fixed Representation** : The model is static; it does not adapt to new contexts or meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* captures semantic relations\n",
    "* dense vectors\n",
    "* low dimensional vectors\n",
    "* its called an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Basic ideas that powers the word2vec model?\n",
    "\n",
    "* words appearing in similar contexts have similar meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/w2v001.png\" alt=\"image description\" width=700 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/w2v002.png\" alt=\"image description\" width=700 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the benefit of a numerical text representation algo that captures semantic meaning?\n",
    "\n",
    "* words with similar meaning will be closer together while plotting their num representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What do you mean by semantic/ meaning?\n",
    "\n",
    "* it is about understanding a deeper meaning and relationships between words\n",
    "* BOW, TFIDF treat words as seperate entities\n",
    "* w2v creates a num rep based on the context in which the word occured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How can word2vec capture or learn the similarities in words based on their meaning?\n",
    "\n",
    "* Using ANNs that takes the text data and extracts meaning and relationship between words\n",
    "$$text \\to shallow \\ ANN \\ (algo) \\to Word2Vec \\ (model) \\to num \\ representation$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Name the techniques that help to boil down the problem statement above viz. seemingly an unsupervised learning problem to a familiar classification(SL) problem?\n",
    "\n",
    "* CBOW - continuous bag of words\n",
    "* skip gram\n",
    "* skip gram with negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Deep ANN|Shallow ANN|\n",
    "|---|--|\n",
    "| Many hidden layers| lesser number hidden of layers|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **CBOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the goal of CBOW technique?\n",
    "\n",
    "The CBOW model aims to **predict a target word based on its context words**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Differentiate between the target word and its context words.\n",
    "\n",
    "* Context words are simply the words surrounding a target word in a sentence, within a specified window size.\n",
    "* Window Size defines how many words around the target word will be considered as context\n",
    "* For instance, if we have a window size of 2, then we look at the 2 words before and the 2 words after the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Illustrate target & context with example.\n",
    "\n",
    "Sentence - \"\"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "target word - say target word is \"fox\"\n",
    "\n",
    "window size - say, 2\n",
    "\n",
    "context words - \"quick\", \"brown\", \"jumps\", \"over\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Is CBOW a supervised learning method?\n",
    "\n",
    "Yes, \n",
    "\n",
    "* The training data consists of context-target pairs: given a set of context words, the model needs to learn to predict the target word\n",
    "*  CBOW model uses error (loss) between the predicted and actual target words to update its weights\n",
    "* learns based on predefined relationship between context words (input) and target words (output), optimizing its weights by minimizing the difference between predicted and actual outputs, just like in typical supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is fed as input into the CBOW neural network ?\n",
    "\n",
    "* the context words\n",
    "* based on window size --> if window size = k, then k words occurring before the target word and k words occurring after the target word are fed as inputs\n",
    "* each of the context words will be fed in the form of binary(0,1 are the only component values) fixed-length vectors\n",
    "* the fixed length of such vectors = the number of unique words in the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain how the words are represented as one-hot vectors.\n",
    "\n",
    "1. determine the vocabulary from the corpus ie. the collection of unique words\n",
    "2. for each word in the vocabulary, assign a unique index; an integer\n",
    "3. find out the size of the vocabulary, say N\n",
    "4. then any word in that vocabulary with index i can be represented using \n",
    "   * a N-component vector\n",
    "   * with i-th component equal to 1\n",
    "   * & rest of the (N-1) components equal to 0\n",
    "\n",
    "   **NOTE** : $1 \\leq i \\leq N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the layers in a CBOW's neural network.\n",
    "\n",
    "1. Input layer\n",
    "2. Embedding layer(hidden)\n",
    "3. Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the purpose of the embedding layer in CBOW?\n",
    "\n",
    "* to convert sparse, high-dimensional one-hot encoded vectors (which are typically very large) into lower-dimensional, dense vectors that can capture semantic relationships between words\n",
    "* For example, words that have similar meanings should have similar vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Explain the dimensionality of the embedding space in the context of CBOW.\n",
    "\n",
    "* it is the number of components in vectors that are generated by the embedding layer\n",
    "* how many features each word vector will have\n",
    "* this will be much less than the size of the corpus vocabulary ie. the number of components in the input one-hot vectors\n",
    "* typically 50, 100, 200 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** In the due course of accomplishing its task of predicting target words using context words, what is the shallow neural network of CBOW trying to learn?\n",
    "\n",
    "* it is trying to learn the vector representations of words in the vocabulary\n",
    "* unlike, trivial high-dimensional, sparse one-hot vector representations, the learned representations will be dense and low dimensional and will capture the semantic relations between the words in the vocabulary\n",
    "* these are also called word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is meant by embedding matrix in the context of CBOW?\n",
    "\n",
    "* Embedding matrix is a 2D matrix with dimension, V×d, where:\n",
    "    * V is the vocabulary size \n",
    "    * d is the embedding dimension \n",
    "* Each row represents the word embedding (vector) for a specific word in the vocabulary\n",
    "* each column represents a dimension in that embedding space\n",
    "* iow. during training, the CBOW tries to create a perfect embedding matrix that captures actual semantic relation between words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Name the connection bw CBOW's input layer & embedding layer?\n",
    "\n",
    "* lookup connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How are inputs fed into the CBOW NN?\n",
    "\n",
    "1. Each context word is represented as a one-hot encoded vector of length 100. If the context has 4 words, there will be 4 separate one-hot vectors for those words.\n",
    "2. Instead of feeding the one-hot vectors directly into the neurons (as in a fully connected layer), each one-hot vector is used to perform an embedding lookup. \n",
    "3. lookup retrieves the corresponding row in the embedding matrix\n",
    "4. The one-hot vector effectively acts as an \"index\" in the vocabulary, pulling out the embedding for each word from the embedding matrix.\n",
    "5. Once the embeddings for each of the 4 context words are retrieved, they are averaged or summed to produce a single vector. This combined vector is then passed to the next layer to predict the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** CBOW's neural network is a **single layer network**. Justify.\n",
    "\n",
    "* In the basic CBOW architecture, there’s often **no hidden layer between the averaged embeddings and the output layer**, making it a single-layer network.\n",
    "* However, some implementations may include a hidden layer after the embedding layer to add non-linearity and improve the model's ability to capture complex relationships in the data. \n",
    "* If a hidden layer is added, the averaged embedding vector would be transformed through this layer using a weight matrix and an activation function (e.g., ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is context vector?\n",
    "\n",
    "* the average of the context word embeddings for any target word is called context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the features of output layer of CBOW NN.\n",
    "\n",
    "* activation function is softmax\n",
    "* number of neurons in the output layer is equal to the vocabulary size\n",
    "* each neuron represents the likelihood of each word being the target word given the context\n",
    "* each neuron in the output layer corresponds to one word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How does the output layer use the context vector for predicting the target word?\n",
    "\n",
    "1. The averaged embedding (or output from the hidden layer if there is one) is multiplied by a weight matrix  and a bias term is added. \n",
    "2. The result is a vector with a score for each word in the vocabulary.\n",
    "3. Applying the softmax function to this vector converts the scores into probabilities, with each probability representing the likelihood of a specific word being the target word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is embedding matrix being created during training?\n",
    "\n",
    "* In some models, like word2vec (CBOW and Skip-gram), the embedding matrix is initialized with random values.\n",
    "* During training, the model learns embeddings by adjusting these values based on the context words surrounding each target word.\n",
    "* embeddings are adjusted to minimize the error in predicting a target word given its context, while Skip-gram does the opposite (predicting context given a target word).\n",
    "* Through multiple training epochs, these embeddings are updated using backpropagation until they capture the relationships between words based on co-occurrence patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Word2Vec cannot capture the context in which a word appears. Give example.\n",
    "\n",
    "* Sentence 1 : \"I went to the bank to take a bath\"\n",
    "* Sentence 2 : \"I went to the bank to apply for a loan\"\n",
    "\n",
    "In both sentences, the word bank has different meaning depending on the context. However word2vec representations of these both banks will be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What features of the word2vec model prevents it from differentiating similar words with context-dependent meanings?\n",
    "\n",
    "* **Single Vector Per Word**:\n",
    "    * generates a single embedding (vector) for each unique word in the vocabulary\n",
    "    * This single embedding captures the overall meaning of the word based on all contexts in which it appears during training.\n",
    "* **Context-Independent Representation**:\n",
    "    * The embedding is averaged over various contexts\n",
    "    * so words with multiple meanings get a \"generalized\" representation that blends different contexts\n",
    "    * In the case of \"bank,\" the model learns a representation that might mix meanings like \"riverbank\" and \"financial institution\" into a single vector\n",
    "    * This depends on \n",
    "        - how often the word appears in the text\n",
    "        - and in what contexts it appears\n",
    "* **Limited Sense Disambiguation**:\n",
    "    * does not handle polysemy (multiple meanings) \n",
    "    * it doesn’t consider the specific sentence-level context each time it encounters the word\n",
    "    * So, \"bank\" in a sentence about a river and \"bank\" in a sentence about finance both get mapped to the same embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Drawbacks of word2vec technique.\n",
    "\n",
    "* doesn't capture the sequence of words in a sentence(thereby the intended expression of the sentence) \n",
    "* This can be solved using recurrent neural networks\n",
    "* this is due to the incapability of shallow neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the neural networks capable of capturing the sequential info in the data.\n",
    "\n",
    "* LSTM\n",
    "* GRU\n",
    "* RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ELMO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding from Language MOdelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** ELMO **model** is created using _____ **algorithm**.\n",
    "\n",
    "* Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the **technique** used by Bi-LSTM for building ELMo model?\n",
    "\n",
    "* Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shallow NN ---> Bi-LSTM (algo)\n",
    " & CBOW ---> Language Modelling (technique) then,\n",
    "\n",
    "word2vec ----> ELMo (Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** ELMo was not used heavily in the industry. Why?\n",
    "\n",
    "* the use of bi-lstm, rnn or gru as they process the sequences taking one token at a time. The number of tokens can be exceptionally huge in real world data. Hence, the process is highly time consuming (an impactful bottleneck in NLP before 2017)\n",
    "* speed of processing sequences is too low\n",
    "* model development is a time consuming process\n",
    "* these models where taken over by the **transformers** algo developed by Google. It revolutionized the whole field of NLP paving way for the brand new idea of Gen AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRANSFORMERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are transformers?\n",
    "\n",
    "* it is a **deep learning model architecture**\n",
    "* a model architecture defines the structural design for organizing and processing data within a neural network\n",
    "* It’s neither a standalone model nor a single algorithm\n",
    "* it serves as a blueprint for building models like GPT, BERT, and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** If I can call a transformer as a modified version of something familiar, what is that familiar something?\n",
    "\n",
    "* transformers are a modified version of RNNs & LSTM networks\n",
    "* transformers keep the sequence-handling strengths of RNNs but replace the sequential processing with a more efficient, attention-based structure\n",
    "\n",
    "**Modifications**\n",
    "\n",
    "1. self-attention mechanism\n",
    "2. Parellel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Parts of a transformer algo.\n",
    "\n",
    "1. Encoder only transformer\n",
    "2. Decoder only transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE 01**\n",
    "\n",
    "MODEL = BERT\n",
    "\n",
    "ALGO = encoder only transformer\n",
    "\n",
    "TECH = Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE 02**\n",
    "\n",
    "MODEL = GPT\n",
    "\n",
    "ALGO = decoder only transformer\n",
    "\n",
    "TECH = Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is meant by long term dependencies?\n",
    "\n",
    "* sometimes to underatand the context we must travel farther form the word\n",
    "\n",
    "Eg. The bank was overcrowded, with people form all across the country comming to worship\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is a possible solution? When was it discovered?\n",
    "\n",
    "* Attention Mechanism, 2015\n",
    "* in order to capture the meaning we dont need to look at all the surrounding words.\n",
    "* just by focusing on some important words we can understand more about each word\n",
    "\n",
    "* AM plays a vital role by learning context-based language rep\n",
    "* it also helps with the long term dependency problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Language Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity :** Predict the next word !!!\n",
    "\n",
    "1. I ___\n",
    "\n",
    "**A :** am / was / have / can / will\n",
    "\n",
    "2. I am ___\n",
    "\n",
    "**A :** Abhinaya / a \n",
    "\n",
    "3. I am learning ___\n",
    "\n",
    "**A :** data science / maths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** The word can be anything. But still we predicted certain specific words only. Why ?\n",
    "\n",
    "* I have good vocabulary\n",
    "* I have good language skills\n",
    "* I know grammar\n",
    "* I am familiar with usage of words\n",
    "* I am aware of what words to use based on context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Imagine a machine having the above skills you mentioned. What possible tasks it can do then ?\n",
    "\n",
    "* auto-complete\n",
    "* text summarizations\n",
    "* QnA\n",
    "* Machine translations\n",
    "* chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** The above tasks are called ____ tasks.\n",
    "\n",
    "* Generative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** A model built on text data is called ____.\n",
    "\n",
    "* Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Language models try to learn what ?\n",
    "\n",
    "* **sequential** relationships between words and sentences of a language in which the text is written\n",
    "* the language in which the text is written\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the basic types of language modelling techniques?\n",
    "\n",
    "1. Auto regressive\n",
    "2. Auto encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Auto-regressive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are auto-regressive lang models trying to do?\n",
    "\n",
    "* They are trained to predict the next roken in a sequence, based on the previous tokens\n",
    "* a mask is applied to full sentence\n",
    "* unidirectional (forward / left to right through a sentence)\n",
    "* also called \"**next-word prediction**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is masking?\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How are sentences represented for this purpose ?\n",
    "\n",
    "```html\n",
    "<s>I am learning language modelling</s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** ChatGPT uses what type of language modelling? Why?\n",
    "\n",
    "Auto-regressive lang modelling. While it writes answers, it writes it word by word viz. nothing but next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Auto-regressive model is a type of RL. Justify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Auto-Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is main idea behind auto encoding technique?\n",
    "\n",
    "* they are trained to reconstruct the oringinal sentence from a corrupted version of the input\n",
    "* Bidirectional\n",
    "* certain words of a sentence are replaced with a special token, usually \"[MASK]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** This method is similar to the familiar fill in the blanks question. Justify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is reconstruction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Large in the word LLM represents what?\n",
    "\n",
    "1. Number of model parameters ie., size of the model\n",
    "2. Amount of data used for training the model\n",
    "3. Both 1 & 2\n",
    "\n",
    "**A :** 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out some companies and the names LLMs developed by them and applications that are built on those models.\n",
    "\n",
    "* Open AI - GPT 1/2/3/4/4o - Chat GPT\n",
    "* Meta - LLama\n",
    "* Google - Gemini\n",
    "* Microsoft - Phi\n",
    "* Anthropic - Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** The idea of training transformer algos using lang modelling technique was introduced in 2017 paper. But it was only in 2020 that these LLMs were developed using this idea?\n",
    "\n",
    "* Time was taken for development of transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give a basic definition of transfer learning.\n",
    "\n",
    "* Using knowledge acquired previously by means of doing some task to solve another related task.\n",
    "\n",
    "Eg. Task 1 - Riding a bicycle & Task 2 - Riding a motorcycle. \n",
    "\n",
    "Here Task 1 gives the ability of balancing 2 wheelers.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Consider a model trained for language translation task, say English to Hindi translation. Can it do text summarization?\n",
    "\n",
    "* No, they are entirely different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Downstream task vs Pre-training task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||Language representation| Captures |\n",
    "|----|----|----|\n",
    "|1| BOW | frequency or word counts|\n",
    "|2| TF-IDF | word importance|\n",
    "|3| word2vec |semantic meaning by learning relationships b/w different words|\n",
    "|4| BERT | word context & sequence|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |Model|Algorithm|Technique|Description|\n",
    "|-----|-----|-----|-----|-----|\n",
    "|1|Word2Vec|shallow ANN|CBOW/Skip gram||\n",
    "|2|Embedding from Language Modelling|Bidirectional Long Short term Memory|Language model|time consuming|\n",
    "|3|Bidirectional Encoder Representations from Transformers |Encoder Only Transformer|Auto-encoding Language modelling||\n",
    "|4|Generative Pre-trained Transformers|Decoder Only Transformer|Auto-regressive Language modelling||\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/vectortechs.jpeg\" alt=\"image description\" width=700 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gemini's AI is free for few initial requests\n",
    "* set up & API key is a bit different than in Open AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q : Which website to visit inorder to play with Google AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q : How to set up the API keys in Google AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q : GitHub repo for Google AI basics?\n",
    "\n",
    "A : Gen AI scratch 2 advance by that ai guy/Google ai walk through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIzaSyD_z6S81cYBq-kJ-KbEYf7ogZeygORJWhY"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
